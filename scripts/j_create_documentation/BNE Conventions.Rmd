---
title: "BNE Conventions"
author: "Sebastian T. Rowland et. al"
date: "7/9/2021"
output: 
  html_document:
    toc: true 
    toc_float: TRUE
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview
## BNE Overview 
 Bayesian Non-parametric Ensemble (BNE) is an ensemble approach in which the 
 weights vary smoothly over space and time according to a Gaussian kernel. In 
 other words, for a particular point in space-time, the weight of an input model
 is informed by the model's performance at nearby points. This approach is 
 particularly well-suited for modeling environmental factors that vary smoothly
 over space, such as temperature and air pollution, as individual models have been 
 have accuracy that varies over time and space. For example, if a model has relatively 
 high performance in New England, but not in the Mid-Atlantic region, BNE will 
 assign heigh weights to that model in New England, but not in the Mid-Atlantic.
 relatively well in New England 
 
## Convention Document Overview
  During BNE team meetings, it became clear that more documentation would support our
  collaboration. This documentation would also help us structure our code 
  so that we can make a package for BNE. We have also found that creating this 
  documentation has identified issues for us to discuss and features for us to 
  explicitly choose. For now (summer 2021), this document is an internal 
  document only for researchers actively working on BNE. Once we develop the package, 
  text from this document will probably go into the package's documentation. 
  Note that some of the structure may need to change when we switch to using 
  a common server. Note that we have two goals: develop the model (and then the package) 
  and apply BNE to the US to create estimates that we can use to answer scientific questions. 
  Note that questions for the group or unsettled points will be in 
  <span style="color: purple;">purple text. </span> 

## Glossary

* *AQS:* Air Quality System. Air pollution monitoring system of the United States 
Environmental Protection Agency. Considered the ground truth for the US-based 
application
* *Base Grid:* A set of areal units used as the base layer that all the input models 
are aggregated to. BNE's predictions will have the same resolution as the base grid. 
Note that the base grid is only used to generate predictions - the base grid is not 
used for training 
* *BNE:* Bayesian Non-parametric Ensemble. The likelihood function of the model and the 
algorithm used to estimate the model parameters.
*CONUS:* Contiguous United States. Also known as the conterminous United States or 
"the Lower 48." Refers to the US territory belonging to 48 states (excluding 
Alaska and Hawa'ii) and Washington, DC. The US-based application is focused only 
on CONUS because the spatial discontinuity over Canada and the oceans causes issues. 
* *Ground-Truth:* The observations considered to be 'true', against which the BNE 
model is trained, also known as empiricial observations. For the CONUS application, 
the AQS data is used as the ground truth. 
* *Helper functions:* functions that help the user go from raw input models to BNE outputs 
and representations (e.g., plots) of BNE outputs. 
* *Input Models:* The individual prediction models that are incorporated into BNE 
to ultimate generate predictions. For the US-based application, input model 
names are based on either the first letters of the model, if the model has a 
formal name, or the initials of the PI of the model. AV: Aaron Von Donkelaar's model; 
GS: Global Burden of Disease; CM: CMAQ-AQS fusion; JS: Joel Schwartz' models; 
CC: CACES model
* *Offset:* Also known as the 'bias', this term is supposed to capture systematic 
bias present in all of the input models. The offset varies smoothly over time 
and space according to a Gaussian process.  
<span style="color: purple;"> Does the offset/bias have the same kernel as the weights? </span> 
* *Package:* Collection of BNE-related functions. Includes BNE itself, functions to 
process input models, functions to process the BNE outputs, documentation, and a 
vignette.
* *PPD: Posterior Predictive Distribution:* The probability distribution of the 
values of a parameter, as estimated by a Bayesian model. The PPD represents our 
understanding of the plausible values of a parameter, after we have updated our 
knowledge using the evidence we collected. As a Bayesian model, BNE yields a PPD 
for each parameter of interest. 
* *Prediction:* Our estimate of a value. Generally refers to the predicted value  
of a pollutant at some location and time. Note that for BNE, prediction does not 
refer to future values, but values within our study period + location. 
* *Predictive Uncertainty:* How wide is the range of plausible values for the concentration 
at a particular location at a given time? The wider the range, the high the uncertainty, 
as there are more possible values for the concentration. So far predictive uncertainty 
has been measured as the standard deviation of the PPD of the concentration. Note that 
predictive uncertainty is given our evidence and our model; if we use a different model, 
then we very well may have a different amount of uncertainty. For example, we have found 
that adding low-performing models (such as MERRA) and changing the kernel size can 
influence uncertainty. This does not mean that the uncertainty is 'wrong,' it just 
means that that is our uncertainty, given our choices. Approaches such as calibration and 
posterior predictive check can be used to evaluate whether the PPD accurately reflects 
our uncertainty, e.g., the 95% credible interval should include 95% of observations.
* *Reproducibility:* The ability for another researcher who is not the main analyst
to generate the same results using the provided code and manuscript and  to re-do 
the analysis on their own, if they had the same data. Reproducibility 
is an important principle for developing robust and transparent science and reliable results. 
Reproducibility supports iterating over code, detecting errors in code, and 
having the methods section of the manuscript reflect the process. 
* *Run:* A single instance of training BNE and generating predictions. In contrast, 
a test would be running BNE multiple times, with different parameters / conditions, 
and comparing results

# Workflow 
## Diagram of BNE 
insert diagram 

## Data Processing 
* Models should first be processed to have uniform column names - in 1_loadData we set uniform column names for our datasets
* To create the training dataset, we spatially joined each model to the EPQ AQS dataset, using the prediction that was closest to each monitor (i.e., nearest neighbors). 
* For more details on the join and how missing data are addressed, see documentation for spatioTemporalJoin
* To create the prediction dataset, we first created a reference grid (based on 1% of the locations of JS), and then joined each prediction dataset by nearest neighbor. 

## Components of BNE Input and Output
* Note that while the predictions themselves represent the estimated 
average concentration across the grid cell or other areal unit, to fit BNE 
we use the grid cell's centroid as the location. 
* Training BNE requires: times & locations of ground-truth observations, the 
ground-truth observations, and the predictions at those times and locations of 
the input models.
* Generating Predictions with BNE requires: the PPD of the BNE parameters and a 
a tidy dataset of all of the input model predictions. The input model predictions 
need to be at the same spatial scale (harmonized), so that the weights can be appropriately 
estimated. For each point, the weights must sum to 1, however if one tried to multiply
each un-harmonized input model with weights at that spatial scale, a more coarse
grid would have more constant weights, whereas a more resolved grid would have 
more varying weights, which would lead to the weights not actually summing to one
once you put them back together.  

# Conventions for Sharing Resources

## Folder Structure 
* Readme file
* Literature
* Data Tracker
    * Spreadsheet of models, relevant references and contacts 
    * literature references for each input model, ground truth, and external validation datasets
* inputs 
    * pollutant (pm25, o3, nox)
        * base_models 
            * annual
                * model
                    * metadata
                    * data files
            * daily
                * model
                    * metadata
                    * data files
        * ground_truth
            * raw
                * annual
                    * metadata
                    * data files
                * daily
                    * metadata
                    * data files
            * formatted
                * annual
                    * data files
                * daily
                    * data files
       * keys
          * data files
       * reference_grids
          * data files
       * training_datasets
          * annual 
              * data file (one file with all time steps)
              * data file with a column for leave-out folds
          * daily
              * data file (one file with all time steps)
              * data file with a column for leave-out folds
       * prediction_datasets
          * annual
              * data files (one per time step)
          * daily
              * data files (one per time step)

    * outputs
        * pollutant (pm25, o3, nox)
            * daily
                * results of run (name describes model features) 
                * log files describing the run 
            * annual
                * results of run (name describes model features) 
                * log files describing the run 
* external_validation_data 
    * raw
        * Descriptions of Data Sources 
        * [EVl Name]_daily_raw
        * [EV Name]_annual_raw
    * formatted
        * [EV Name]_daily_formatted
        * [EV Name]_annual_formatted
* ancillary_data
* scripts 
    0) functions
    1) unstable_functions
    a) set_up 
    b) format_base_models 
    c) format_ground_truth 
    d) format_training_data
    e) format_external_validation_data 
    f) run_BNE 
    g) summarize_BNE_outputs
    h) perform_cross_validation 
    i) perform_external_validation
    j) create_documentation
    k) depreciated 
        * former_code
        * internal_workflow
* Researcher-Specific Folders
    i) Project-Specific Scripts 
    ii) Project-Specific Data 
    iii) Project-Specific Outputs

## Projection for CONUS Application 
* For the CONUS application we will use the US National Atlas Equal Area projection. 

## Github
* We will share our code over GitHub, and push updates regularly. If multiple researchers are working on common scripts (i.e., in the Scripts folders), they should communicate to coordinate updating the code
* At the start of the work day, pull in the most recent version of the whole repo. 
* At the end of the day, push that version to GitHub. 
* If you create a file and you are still working on it, and you do not want anyone else to modify it, you have three options: a) exclude the file when you push, b) put the file in a personal project folder, or c) begin the file with your initials (this option is most useful for functions) 
* If you are working on a file and you do not want anyone to modify it, either put it in a personal project folder, or begin the file with your initials ('lock the code'). 
* If you need to modify code authored by another researcher, but the code is 'locked' (file name begins with their initials), you can either make a copy of the code and modify that copy, or directly ask the author for permission (and explain what changes you plan to make).
* We follow this locking convention to reduce the probability that two researchers end up with different versions of a code, which makes version control messy. 


# Conventions for Storing Data 

## Folder Names
* Ideally folder names and path names are short, and do not contain the same information as the file names. Folder names should be modular and follow parallel prefix and suffix conventions

## Dataset File Names 
* Dataset File names should contain as much information as necesary so that any researcher in the project could know its contents. For the time being, while we still do not have a server and the sometimes researchers email each other data, we should err on the side of putting extra information in the file names. Once we are on the server, potentially file names could get shorter. 
* For now, input model names will follow the convention of [Model Initials] _ [Temporal Resolution] _ [Time Step] _ [Processing Stage]. For example, JS_daily_01012010_raw.csv 
* For the BNE runs, so far, I store the outputs as a single file, with the naming convention: [Input Model Initials] _ [Temporal Resolution] _ [Year(s) Covered] _ [Spatial Kernel Size] _ [Temporal Kernel Size] _ [cluster]. For example, AVGSCM_annual_20102015_3.5_0.5_all.csv. IF the kernel sizes get established, we can drop them from the name.
  <span style="color: purple;"> Maybe the raw data files should keep their original names? </span> 

## Variable Names 
* Variables should contain the minimum amount of information necessary to distinguish columns within the dataframe. Given the file name and folder, it should be clear what each variable represents.
* Variable names can be modular, with suffixes and prefixes that designate the differences between the columns, separated by underscores

## Formats
* Dates should be stored as stringes to avoid Excel mangling dates. 
* If using a date-time variable format, store as UTC. (For BNE itself, the dates arejust an index and we do not need Posit.x format or similar. When wrangling the data, we might need such formats) For health analyses, use local time zones, as the local time is generally more relevant for health risks (e.g., during local daylight savings time diurnal patterns will shift relative to UTC)
* Large datasets should be saved as fst, <span style="color: purple;">though MatLab might not be able to open or save fsts. </span> When we have the server, this is less important. 
* Although raw data might include multiple timesteps,  we will first split the input model predictions  by timestep to facilitate combining the 
predictions later. 

## Units
* We should agree on a common set of units, and then one step in processing the input models in converting them to the common units 
* PM_2.5_ should be stored as Î¼g/m^3
* Other environmental data should be in SI units

## Coordinate System
* Since pretty much all of the original models are trained using latitude-longitude coordinates, when training BNE we should also use location information stored as latitude-longitude coordinates. 
* <span style="color: purple;"> While doing area-weighted averaging would be slightly more accurate if we projected to a meter-based coordinate system, it is more sensible to use the same coordiante system for preparing the data as we use for running BNE. </span> 

# Conventions for Coding 

## Programming Languages
* Python, R, and Matlab 
* <span style="color: purple;"> Will the published package be only in R (aka will we need a version of BNE in R?  </span> 

## Style
* **Code Files**
  * We have different rules for naming function files and non-function files 
  * *Function Code Files*
    * (initials_) [directory-number] _ [file-name] .R
    * The directory number is either 1 for unstable functions still under development or 0 for a stable function whose development is mostly finalized
    * The file name should use camelCase. The file name should align with the name of the function; should be informative and begin with a verb.
    * Functions from packages (not base R) should have explicit function calls, i.e., call the package that the function comes from, like so: package::function (dplyr::select()). This will be required for the package version and avoid namespace overlap issues.    
  * *Non-Function Code Files*
    * (initials_) [directory-letter] _ [step-number] _ [file-name] .R
    * initials. Use your initials if you do not want anyone to modify the code (eg LGC_)
    * directory designates the section. Lower case letters. 
    * step indicates the order of running code. Paded to have two digits. 
    * file name describes the code, should begin with a verb. Use lower case letters and underscores.

* for objects: everything should be camelCase, can add a punctuation-based extension (period in R; underscore in Python) to denote slight differences in objects, e.g. monitorLocations and monitorLocations.sf to distinguish betweer the dataframe and simple features versions of the object. 
* for column names: all lower case letters, with underscores to separate words, to avoid conflicts with Python

* Be consistent with similar types of objects. 
* Object names should align with the terminology used in the manuscript (and vice-versa)
* Section names should align with the subsections of the Methods section 
* Write code (eg path names) such that Windows and Macs can handle the code without modifications (aka use the here package)
* In general, I (Sebastian) try to follow Hadley Wickham's style guide: 

## Workflow for Finalizing Functions for Package 
* Functions should live in the 1_unstable_functions folder while they are under development
* For functions that we use for our CONUS application but not generalizable to other applications (e.g., loadData), we will also include those in the package, but include a suffix 'H' (H for helper)
* Once a function is stable enough (it doesn't require frequent bug fixes, logic overhauls, or major additional features), we can relocate it to the 0_functions folder after under going function review:
    1. Standardize style in accordance with code style described throughout this document. Importantly, this should also include conventions for calling packages, i.e. package::function().
    2. Write roxygen documentation for the function. 
    3. Incorporate error handling.
    4. Make it abstract - argument names etc should be generalizable to other settings with different datasets
    5. Have a team member other than the primary author of the code review the function for efficiency, style, reproducibility, etc. 
      * Incorporate option for progress bar where appropriate 

## General 
* Include a mega script that sources all of the scripts, in order, to go from raw data to final product
* Predictions should be treated as areal units, because predictions represent the 
estimate of the average concentration across the grid, not the point estimate at 
the centroid of the grid. 
* Monitor readings (like AQS) should be treated as points because they just capture what is happening at the location of the monitor. 
* This implies that nearest-neighbor or spatial overlap are appropriate for combining the input model predictions and the ground truth data, but area-weighted averaging is more appropriate for combining the input models into the combined input dataset that we use to generate BNE predictions. Since nearest neighbor is more efficient (especially with the JS model), for now we will use nearest neighbors. 

# Best Practices for Reproducibility 

## Documentation of Data Sources
* Each input model should be recorded in the Input Model Spreadsheet
* For each input model, we should have at least one manuscript describing the model. If the version of the model we have differs in any way from the version in the paper the differences should be noted. 
* For each raw data file, whether input model, ground truth data, or other data, thereshould be a .txt file (a readme) describing how the data was collected and when. Writing this file at the time of data collection will minimize errors. 
* Description of the steps for processing the data

## Commenting in Code 
* At a minimum, significant chunks of code should be commented so that someone familiar with the project could follow the logic. Include data dictionaries for the first time you read in a dataset. 

## Language Alignment
* We should use the same terms in the manuscript, code, figures, and documentation. 
* When possible, avoid using interchangeable terms - just use one term per concept.
* <span style="color: purple;"> Although in principle this consistency is very  valuable, in the real world it could be a detriment as different fields (ML, atmospheric chemistry, epidemiology) use different terms for the same concept. How do we strike a balance? </span> 

## Code Review 
* Prior to publication, all code contributing to the manuscript should undergo code review by a peer. Specifics of the code review can be decided by main analyst and the reviewer. Code reviewer will become a co-author on the paper.

## Start-to-Finish Principle
* Raw data should be as close to the original as possible, and documented so that someone else could collect that data.
* Code should be organized such that the user can go from the start to the end of the project if all the intermediate products were deleted. The data downloads & gathering should be described such that if the raw data were deleted, the user could re-collect the data (barring input models that were directly emailed to the user). Due to the nature of the project, some sections or pre-processing may only occur on certain computers; researchers can clearly identify where the missing steps took place.
* ReadMe's and bash scripts can help keep track of the BNE runs

## To-Do List 
* This is just a list of tasks to eventually do, as we think of them. 
