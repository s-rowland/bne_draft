{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0m8xce2x8ZC",
    "outputId": "817c403f-83a9-4401-9c9b-68c2439f35ff"
   },
   "outputs": [],
   "source": [
    "import os as os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import cholesky, cho_solve\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from sklearn.neighbors import BallTree\n",
    "import rff\n",
    "import rff_time\n",
    "import datetime\n",
    "import random as rd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZSLbKXbx1ML"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZLDm7Mj5yWLD"
   },
   "outputs": [],
   "source": [
    "def get_training_data_one_time(filePath,obsVar = 'obs_pm2_5', baseModels = ['av_pred', 'gs_pred','cmaq_outs_pred', 'js_pred', 'caces_pred']):\n",
    "\t\"\"\"\n",
    "\tReads the training file YEARLY_TRAINING_FILE and returns the training data along with model predictions on training data.\n",
    "\t\"\"\"\n",
    "\t# Read the training data file using pandas into a DataFrame.\n",
    "    # training data for just one year\n",
    "\ttraining_data = pd.read_csv(filePath)\n",
    "\tprint('Columns of the training file = {}'.format(training_data.columns.values))\n",
    "\tprint('Shape of the training file = {}'.format(training_data.shape))\n",
    "\t# Shuffle the data row-wise\n",
    "\ttraining_data = training_data.sample(frac=1.0)\n",
    "\t# (n, 2) numpy array.\n",
    "    # we do not have time because this is annual data\n",
    "\tX_train = training_data[['lat', 'lon']].values\n",
    "\t# (n, 1) numpy array.\n",
    "\ty_train = training_data[[obsVar]].values\n",
    " # assert checks whether a statement is true and returns an message if it is false\n",
    "\tassert X_train.shape[0] == y_train.shape[0], \"The number of rows in X_train, {0} and y_train don't match {1}\".format(X_train.shape[0], y_train.shape[0])\n",
    "\t# Models' predictions on training data, as a (n, 3).\n",
    "    # we will need to adjust this if we have other predictors\n",
    "\tmodels_on_train = training_data[baseModels].values\n",
    "\tassert models_on_train.shape[0] == X_train.shape[0], \"The number of rows in models_on_train, {0} and X_train don't match {1}\".format(models_on_train.shape[0], X_train.shape[0])\n",
    "\treturn X_train, y_train, models_on_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_multi_time(filePath, obsVar = 'obs_pm2_5', baseModels = ['av_pred', 'gs_pred','cmaq_outs_pred', 'js_pred', 'caces_pred']):\n",
    "\t\"\"\"\n",
    "\tReads the training file TRAINING_FILE and returns the training data along with model predictions on training data.\n",
    "\t\"\"\"\n",
    "\t# Read the training data file using pandas into a DataFrame.\n",
    "\tdtype_dict = {'year':str, 'month':str, 'day':str} # Read these as string, and others as default\n",
    "\ttraining_data = pd.read_csv(filePath, dtype = dtype_dict)\n",
    "\tprint('Columns of the training file = {}'.format(training_data.columns.values))\n",
    "\tprint('Shape of the training file = {}'.format(training_data.shape))\n",
    "\t# Throw negative or very large outlier values\n",
    "    # data curation should take place elsewhere\n",
    "\ttraining_data = training_data[training_data['obs_pm25'] > 0]\n",
    "\ttraining_data = training_data[training_data['obs_pm25'] < 40]\n",
    "\t# Throw non-conterminous states\n",
    "\ttraining_data = training_data[training_data['lat'] < 49.5]\n",
    "\ttraining_data = training_data[training_data['lat'] > 24.5]\n",
    "\ttraining_data = training_data[training_data['lon'] < -66.5]\n",
    "\ttraining_data = training_data[training_data['lon'] > -125]\n",
    "\t# Shuffle the data row-wise\n",
    "\ttraining_data = training_data.sample(frac=1.0)\n",
    "\t# (n, 3) numpy array\n",
    "\tX_train = training_data[['lat', 'lon']].values\n",
    "\tdates = training_data[['year', 'month', 'day']].values\n",
    "\tdates_combined = []\n",
    "\tn = dates.shape[0]\n",
    "\tfor i in range(n):\n",
    "\t\tdates_combined.append('{0}-{1}-{2}'.format(dates[i,0], dates[i,1], dates[i,2]))\n",
    "\tdates_combined = np.array(dates_combined)\n",
    "\tstart_date = sorted(dates_combined)[0]\n",
    "\t#date_start = np.datetime64(start_date)\n",
    "\tdate_start = np.datetime64('2010-01-01')\n",
    " # dates_delta is the julian day\n",
    "\tdates_delta = np.zeros(shape=(n,))\n",
    "\tfor i in range(n):\n",
    "\t\tdates_delta[i] = np.datetime64(dates_combined[i]) - date_start\n",
    "\t\tdates_delta[i] = dates_delta[i] #% 365 + 10 * np.floor(dates_delta[i] / 365)\n",
    "\tX_train = np.column_stack((X_train, dates_delta))\n",
    "\t# (n, 1) numpy array.\n",
    "\ty_train = training_data[[obsVar]].values\n",
    "\tassert X_train.shape[0] == y_train.shape[0], \"The number of rows in X_train, {0} and y_train don't match {1}\".format(X_train.shape[0], y_train.shape[0])\n",
    "\t# Models' predictions on training data, as a (n, L) for L=6.\n",
    "\tmodels_on_train = training_data[baseModels].values\n",
    "\tassert models_on_train.shape[0] == X_train.shape[0], \"The number of rows in models_on_train, {0} and X_train don't match {1}\".format(models_on_train.shape[0], X_train.shape[0])\n",
    "\tnp.save(RESULTS_DIR + '/X_train.npy', X_train) # Read as X_train = np.load(RESULTS_DIR + '/X_train.npy')\n",
    "\tnp.save(RESULTS_DIR + '/y_train.npy', y_train)\n",
    "\tnp.save(RESULTS_DIR + '/models_on_train.npy', models_on_train)\n",
    "\treturn X_train, y_train, models_on_train, start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_data(filePath, baseModels = ['av_pred', 'gs_pred','cmaq_outs_pred', 'js_pred', 'caces_pred'], sampleFrac = 1, timeStamp = 0, start_date = '2010-01-01', pred_date='2010-01-01'):\n",
    "\t\"\"\"\n",
    "\tReads the prediction file YEARLY_PREDICTION_FILE and returns\n",
    "\tlocations : (m, 2) numpy array\n",
    "\tpredictions : (m, L) numpy array\n",
    "\t\"\"\"\n",
    "\tpred_data = pd.read_csv(filePath)\n",
    "\tif timeStamp != 0:\n",
    "\t\tpred_data = pred_data[pred_data['time'] == timeStamp]\n",
    "\tprint('Columns of the predictions file = {}'.format(pred_data.columns.values))\n",
    "\tprint('Shape of the predictions file = {}'.format(pred_data.shape))\n",
    "\tprint('After sampling...')\n",
    "\tpred_data = pred_data.sample(frac=sampleFrac)\n",
    "\tprint('Shape of the predictions file = {}'.format(pred_data.shape))\n",
    "\tlocations = pred_data[['lat', 'lon']].values\n",
    "\tdate_start = np.datetime64(start_date)\n",
    "\tdate_pred = np.datetime64(pred_date)\n",
    "\tdate_pred_num = date_pred - date_start\n",
    "\tdates_delta = [date_pred_num] * locations.shape[0]\n",
    "\tdates_delta = pd.to_timedelta(dates_delta).days\n",
    "\tlocations = np.column_stack((locations, dates_delta))\n",
    "\tpredictions = pred_data[baseModels].values\n",
    "\treturn locations, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hessian(Z_train, models_on_train, C, model_avg, error, sigmasq, lmbda=0.1):\n",
    "\t\"\"\"\n",
    "\tComputes the D(L+1) x D(L+1) Hessian.\n",
    "\tFor L = 3, it looks like as follows:\n",
    "\t-----------------\n",
    "\t|w_0|0,1|0,2|0,3|\n",
    "\t-----------------\n",
    "\t|1,0|w_1|1,2|1,3|\n",
    "\t-----------------\n",
    "\t|2,0|2,1|w_2|2,3|\n",
    "\t-----------------\n",
    "\t|3,0|3,1|3,2|w_3|\n",
    "\t-----------------\n",
    "\tw_l denotes the DxD block nabla_{w_l}^2\n",
    "\tl,j denotes the DxD block nabla_{w_l} nabla_{w_j}\n",
    "\n",
    "\t# Inputs\n",
    "\tZ_train : (n, D) numpy array containing n projected data points\n",
    "\tmodels_on_train : (n, L) numpy array containing the model predictions on the training inputs\n",
    "\tC : (n, L) numpy array containing probabilitic weights of each model. Each row sums to 1\n",
    "\tmodel_avg = (n, 1) numpy array containing the models' average prediction obained from models_on_train and C\n",
    "\terror : (n, 1) numpy array containing the difference between the true values and ensemble prediction\n",
    "\tsigmasq : variance of the noise\n",
    "\tlmbda : the prior on the weights is N(0, 1/lmbda I_D)\n",
    "\n",
    "\t# Output\n",
    "\t\"\"\"\n",
    "\tn, D = Z_train.shape\n",
    "\tL = models_on_train.shape[1]\n",
    "\t# The Hessian initialized to 0.\n",
    "\tH = np.zeros(shape=(D*(L+1), D*(L+1)))\n",
    "\t# nabla_{w_0}^2 block\n",
    "\tnabla_0_0 = - (1/sigmasq) * np.matmul(Z_train.T, Z_train) - lmbda * np.identity(D)\n",
    "\t# Put nabla_0_0 in the top DxD block of H\n",
    "\tH[:D, :D]= nabla_0_0\n",
    "\t# (n, L) matrix storing the difference between a model prediction and models' average\n",
    "\tmodels_diff = models_on_train - model_avg\n",
    "\t# nabla_{w_l} nabla_{w_0} blocks\n",
    "\tfor l in range(L):\n",
    "\t\tnabla_l_0 = - (1/sigmasq) * np.matmul(Z_train.T, np.multiply(Z_train, np.multiply(C[:, l, None], models_diff[:, l, None])))\n",
    "\t\t# Put nabla_l_0 in the corresponding positions\n",
    "\t\tH[(l+1)*D:(l+2)*D, :D] = nabla_l_0\n",
    "\t\tH[:D, (l+1)*D:(l+2)*D] = nabla_l_0\n",
    "\t# nabla_{w_l}^2 blocks\n",
    "\tfor l in range(L):\n",
    "\t\tfactor = np.multiply(C[:, l, None], np.multiply(models_diff[:, l, None], np.multiply(C[:, l, None], models_diff[:, l, None]) + 2 * np.multiply(C[:, l, None], error) - error))\n",
    "\t\tnabla_l_l = -lmbda * np.identity(D) - (1/sigmasq) * np.matmul(Z_train.T, np.multiply(Z_train, factor))\n",
    "\t\t# Put nabla_l_l in H\n",
    "\t\tH[(l+1)*D:(l+2)*D, (l+1)*D:(l+2)*D] = nabla_l_l\n",
    "\t# nabla_{w_j} nabla_{w_l} blocks\n",
    "\tfor j in range(L):\n",
    "\t\tfor l in range(j+1, L):\n",
    "\t\t\tfactor = np.multiply(C[:, j, None], np.multiply(C[:, l, None], np.multiply(models_diff[:, j, None], models_diff[:, l, None]) + np.multiply(error, models_diff[:, l, None]) + np.multiply(error, models_diff[:, j, None])))\n",
    "\t\t\tnabla_j_l = - (1/sigmasq) * np.matmul(Z_train.T, np.multiply(Z_train, factor))\n",
    "\t\t\t# Put nabla_j_l in H\n",
    "\t\t\tH[(j+1)*D:(j+2)*D, (l+1)*D:(l+2)*D] = nabla_j_l\n",
    "\t\t\tH[(l+1)*D:(l+2)*D, (j+1)*D:(j+2)*D] = nabla_j_l\n",
    "\treturn H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bne(X_train, y_train, models_on_train, D=500, h=20.0, ht=20.0, sigmasq=-1.0, lmbda=1.0, lmbda0=1.0):\n",
    "\t\"\"\"\n",
    "\tTrains the RFF approximation of BNE using gradient ascent.\n",
    "\n",
    "\t# Inputs\n",
    "\tX_train : (n, d) numpy array containing the \"raw\" data inputs\n",
    "\ty_train : (n, 1) numpy array containing the monitor observations\n",
    "\tmodels_on_train : (n, L) numpy array containing the model predictions on the training inputs\n",
    "\tD : Number of RFF features\n",
    "\th : length scale for the Gaussian kernel exp(-|x-y|^2 / (2 * h)). The corresponding spectral measure is N(0, 1/h I_d)\n",
    "\tht : length scale for time\n",
    "\tsigmasq : variance of the noise\n",
    "\tlmbda : the prior on the weights is N(0, 1/lmbda I_D)\n",
    "\t\"\"\"\n",
    "\tn, d = X_train.shape\n",
    "\tL = models_on_train.shape[1]\n",
    "    # change this line if doing spatial-only model\n",
    "\tnp.random.seed(0)\n",
    "\trr = rff_time.RFF_TIME(X_train, D=D, h=h, ht=ht)\n",
    "\t# SNR of 8\n",
    "\tif sigmasq < 0:\n",
    "\t\tsigmasq = np.var(y_train) / 8\n",
    "\t# (n, D) numpy array containing the \"projected\" data inputs\n",
    "\tZ_train = rr.get_Z(X_train)\n",
    "\t# Initialize the weights\n",
    "\tnp.random.seed(0)\n",
    "\tw0 = np.random.normal(size=(D, 1))\n",
    "\tnp.random.seed(0)\n",
    "\tW = np.random.normal(size=(D, L))\n",
    "\t# Hyperparameters for the gradient ascent\n",
    "\tMAX_ITER = 20\n",
    "\tMAX_STEP_SIZE = 0.1\n",
    "\tBATCH_SIZE = 5000\n",
    "\tSPLITS = int(np.ceil(n / BATCH_SIZE))\n",
    "\tLL = []\n",
    "\tfor it in range(1, MAX_ITER+1):\n",
    "\t\tstep_size = MAX_STEP_SIZE / np.sqrt(it)\n",
    "\t\tfor b in range(SPLITS):\n",
    "\t\t\t# Construct (BATCH_SIZE, D) matrix used for training\n",
    "\t\t\tZ = Z_train[b*BATCH_SIZE : min((b+1)*BATCH_SIZE, n)]\n",
    "\t\t\t# (BATCH_SIZE, 1)\n",
    "\t\t\ty = y_train[b*BATCH_SIZE : min((b+1)*BATCH_SIZE, n)]\n",
    "\t\t\t# (BATCH_SIZE, L)\n",
    "\t\t\tmodels = models_on_train[b*BATCH_SIZE : min((b+1)*BATCH_SIZE, n)]\n",
    "\t\t\t### Update W ###\n",
    "\t\t\t# Fourier GP approximation of shape (BATCH_SIZE, L)\n",
    "\t\t\tG = np.matmul(Z, W)\n",
    "\t\t\t# Each row sums to 1. (BATCH_SIZE, L) shape\n",
    "\t\t\tC = softmax(G, axis=1)\n",
    "\t\t\t# At each point get the model average. Shape (BATCH_SIZE, 1)\n",
    "\t\t\tmodel_avg = np.sum(np.multiply(C, models), axis=1, keepdims=True)\n",
    "\t\t\t# Bias term. Shape (BATCH_SIZE, 1)\n",
    "\t\t\tbias = np.matmul(Z, w0)\n",
    "\t\t\t# Error. (BATCH_SIZE, 1)\n",
    "\t\t\terror = y - model_avg - bias\n",
    "\t\t\t# Compute gradient\n",
    "\t\t\tgradient_W = (1/sigmasq) * np.matmul(Z.T, np.multiply(C, np.multiply(np.tile(error, L), models - model_avg))) - lmbda * W\n",
    "\t\t\tW = W + step_size * gradient_W\n",
    "\t\t\t### Update w0 ###\n",
    "\t\t\t# Fourier GP approximation of shape (BATCH_SIZE, L)\n",
    "\t\t\tG = np.matmul(Z, W)\n",
    "\t\t\t# Each row sums to 1. (BATCH_SIZE, L) shape\n",
    "\t\t\tC = softmax(G, axis=1)\n",
    "\t\t\t# At each point get the model average. Shape (BATCH_SIZE, 1)\n",
    "\t\t\tmodel_avg = np.sum(np.multiply(C, models), axis=1, keepdims=True)\n",
    "\t\t\t# Error. (BATCH_SIZE, 1)\n",
    "\t\t\terror = y - model_avg - bias\n",
    "\t\t\t# Compute gradient\n",
    "\t\t\tgradient_w0 = (1/sigmasq) * np.matmul(Z.T, error) - lmbda0 * w0\n",
    "\t\t\tw0 = w0 + step_size * gradient_w0\n",
    "\t\t# Compute log joint likelihood\n",
    "\t\tif it % 10 == 0:\n",
    "\t\t\tprint('Iteration: {}'.format(it))\n",
    "\t\t\tG = np.matmul(Z_train, W)\n",
    "\t\t\tC = softmax(G, axis=1) # rescale the weights \n",
    "\t\t\tmodel_avg = np.sum(np.multiply(C, models_on_train), axis=1, keepdims=True) # compute model average\n",
    "\t\t\tbias = np.matmul(Z_train, w0) # the key thing is that we only update the bias every 10 iterations\n",
    "\t\t\terror = y_train - model_avg - bias\n",
    "\t\t\tLL.append((-1/(2*sigmasq)) * np.sum(error**2) - (lmbda/2) * np.sum(W**2) - (lmbda0/2) * np.sum(w0**2))\n",
    "\t# Compute the covariance matrix of the Laplace approximation\n",
    "\tG = np.matmul(Z_train, W)\n",
    "\tC = softmax(G, axis=1)\n",
    "\tmodel_avg = np.sum(np.multiply(C, models_on_train), axis=1, keepdims=True)\n",
    "\tbias = np.matmul(Z_train, w0)\n",
    "\terror = y_train - model_avg - bias\n",
    "\tH = compute_hessian(Z_train, models_on_train, C, model_avg, error, sigmasq, lmbda)\n",
    "\t# Covariance matrix is the inverse of the negative of the Hessian evaluated at MAP\n",
    "\tCOV = np.linalg.inv(-H)\n",
    "\treturn W, w0, COV, rr, LL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(locations, predictions, W, w0, rr):\n",
    "\t\"\"\"\n",
    "\tGet BNE predictions at locations.\n",
    "\n",
    "\t# Inputs\n",
    "\tlocations : (m, 3) numpy array # location in space and time\n",
    "\tpredictions : (m, L) numpy array\n",
    "\tW : (D, L) numpy array\n",
    "\tw0 : (D, 1) numpy array\n",
    "\trr : RFF object\n",
    "\tm : number of points to predict on\n",
    "\t# Returns\n",
    "\t\"\"\"\n",
    "\t# (m, D) shape\n",
    "\tZ_pred = rr.get_Z(locations)\n",
    "\t# GP Fourier approximation of shape (m, L)\n",
    "\tG = np.matmul(Z_pred, W)\n",
    "\t# Each row sums to 1. (m, L) shape\n",
    "\tC = softmax(G, axis=1)\n",
    "\t# At each location in locations, get model average. Shape (m, 1)\n",
    "\tmodel_avg = np.sum(np.multiply(C, predictions), axis=1, keepdims=True)\n",
    "\t# Bias term. Shape (m, 1)\n",
    "\tbias = np.matmul(Z_pred, w0)\n",
    "\tbne_pred = model_avg + bias\n",
    "    # C is the properly-sclaed weighted, model_avg is the core ensemble component \n",
    "    # bias is the systematic bias term, and bne_pred is the actual predictions\n",
    "\treturn C, model_avg, bias, bne_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty(locations, predictions, W_map, w0_map, COV, rr, mc_samples=500):\n",
    "\tm, L = predictions.shape\n",
    "\tD = W_map.shape[0]\n",
    "\tW_samples = np.empty(shape=(mc_samples, m, L))\n",
    "\tmodel_avg_samples = np.empty(shape=(mc_samples, m))\n",
    "\tw0_samples = np.empty(shape=(mc_samples, m))\n",
    "\tbne_pred_samples = np.empty(shape=(mc_samples, m))\n",
    "\t# (mc_samples, D*(L+1))\n",
    "\tnp.random.seed(0)\n",
    "\tW_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n",
    "\tfor s in range(mc_samples):\n",
    "\t\tif s % 50 == 0:\n",
    "\t\t\tprint(s)     \n",
    "\t\tW = np.reshape(W_w0_samples[s, :D*L], newshape=(D, L))\n",
    "\t\tw0 = np.reshape(W_w0_samples[s, D*L:], newshape=(D, 1))\n",
    "\t\tW_samp, model_avg_samp, w0_samp, bne_pred_samp = get_predictions(locations, predictions, W, w0, rr)  \n",
    "\t\tfor l in range(L):\n",
    "\t\t\tW_samples[s, :, l] = W_samp[:, l] \n",
    "\t\tmodel_avg_samples[s, :] = model_avg_samp[:, 0] \n",
    "\t\tw0_samples[s, :] = w0_samp[:, 0] \n",
    "\t\tbne_pred_samples[s, :] = bne_pred_samp[:, 0] \n",
    "\t# calculate summary metrics \n",
    "\tW_mean = np.mean(W_samples, axis=(0))\n",
    "\tW_std = np.sqrt(np.var(W_samples, axis=0))\n",
    "\tmodel_avg_mean = np.mean(model_avg_samples, axis=0)\n",
    "\tmodel_avg_std = np.sqrt(np.var(model_avg_samples, axis=0))\n",
    "\tw0_mean = np.mean(w0_samples, axis=0)\n",
    "\tw0_std = np.sqrt(np.var(w0_samples, axis=0))\n",
    "\tbne_preds_mean = np.mean(bne_pred_samples, axis=0)\n",
    "\tbne_preds_std = np.sqrt(np.var(bne_pred_samples, axis=0))\n",
    "\t# combine into a nice dataframe\n",
    "\tbne_out = np.column_stack((model_avg_mean, model_avg_std, w0_mean, w0_std, bne_preds_mean, bne_preds_std))\n",
    "\tbne_out = np.column_stack((W_mean, W_std, bne_out))\n",
    "\tcolNames = ['w_mean_av', 'w_mean_gs', 'w_mean_cm', 'w_mean_js', 'w_mean_cc', 'w_sd_av', 'w_sd_gs', 'w_sd_cm', 'w_sd_js', 'w_sd_cc', 'ens_mean', 'ens_sd','bias_mean', 'bias_sd', 'pred_mean', 'pred_sd']\n",
    "\tbne_out_df = pd.DataFrame(bne_out, columns = colNames)\n",
    "#C, model_avg, bias, bne_pred\n",
    "\n",
    "# save to results directory\n",
    "\t\n",
    "\t# Compute the mean also\n",
    "\treturn bne_preds_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cv(X_train, y_train, models_on_train, D=500, h=10.0, ht=10.0, lmbda=1.0, splits=20):\n",
    "\tprint(datetime.datetime.now())\n",
    "\tn, d = X_train.shape\n",
    "\tchunk = int(n/splits)\n",
    "\tmae = []\n",
    "\trmse = []\n",
    "\tcoverage = []\n",
    "\tfor split in range(splits):\n",
    "\t\tprint('Split: {0}  at {1}'.format(split, datetime.datetime.now()))\n",
    "# we can just add filter statements, and a column for leaveOutfold\n",
    "\t\tflag = np.full((n,), True)\n",
    "\t\tflag[split*chunk : min((split+1)*chunk, n)] = False\n",
    "\t\tX_train1 = X_train[flag]\n",
    "\t\tX_train2 = X_train[np.invert(flag)]\n",
    "\t\ty_train1 = y_train[flag]\n",
    "\t\ty_train2 = y_train[np.invert(flag)]\n",
    "\t\tmodels_on_train1 = models_on_train[flag]\n",
    "\t\tmodels_on_train2 = models_on_train[np.invert(flag)]\n",
    "\t\tprint('D = {}'.format(D))\n",
    "\t\tprint('h = {}'.format(h))\n",
    "\t\tprint('ht = {}'.format(ht))\n",
    "\t\tprint('lmbda = {}'.format(lmbda))\n",
    "\t\tW, w0, COV, rr, LL = fit_bne(X_train1, y_train1, models_on_train1, D=D, h=h, ht=ht, sigmasq=-1.0, lmbda=lmbda)\n",
    "\t\tbne_pred, model_avg, bias = get_predictions(X_train2, models_on_train2, W, w0, rr)\n",
    "\t\t# Errors\n",
    "\t\tmae.append(np.sum(np.abs(y_train2 - bne_pred))/y_train2.shape[0])\n",
    "\t\trmse.append(np.sqrt(np.sum((y_train2 - bne_pred)**2)/y_train2.shape[0]))\n",
    "\t\t# Coverage\n",
    "\t\temp_std = get_uncertainty(X_train2, models_on_train2, W, w0, COV, rr, mc_samples=100)\n",
    "\t\tcovered = 0\n",
    "\t\tfor i in range(y_train2.shape[0]):\n",
    "\t\t\tif y_train2[i,0] >= bne_pred[i,0] - 2 * emp_std[i] and y_train2[i,0] <= bne_pred[i,0] + 2 * emp_std[i]:\n",
    "\t\t\t\tcovered += 1\n",
    "\t\tcoverage.append( covered / y_train2.shape[0])\n",
    "\tprint(datetime.datetime.now())\n",
    "\treturn mae, rmse, coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit BNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ABs_TxXf1ke"
   },
   "source": [
    "## Name Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "m1IShAoGxsRk"
   },
   "outputs": [],
   "source": [
    "# combined training dataset location\n",
    "trainAnnualspt = 'training_datasets/training_avgscmjscc_all.csv'\n",
    "trainAnnualsp = 'training_datasets/training_annual_2010_all.csv'\n",
    "predAnnualsp = \"prediction_datasets/predictions_annual_2010_all.csv\"\n",
    "\n",
    "trainDaily = 'training_datasets/dailyTrainingData_2010-2015.csv'\n",
    "predDaily = \"prediction_datasets/daily_pred_2010-01-01_small.csv\"\n",
    "\n",
    "RESULTS_DIR = \"./bne_out\"\n",
    "\n",
    "baseModelSet = ['pred_av', 'pred_gs','pred_cmaq_outs', 'pred_js', 'pred_caces']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of the training file = ['ref_id' 'lat' 'lon' 'year' 'month' 'day' 'obs_pm25' 'pred_js'\n",
      " 'cmaq_ins_pred' 'pred_cmaq_outs' 'pred_gs' 'pred_caces' 'pred_av']\n",
      "Shape of the training file = (586777, 13)\n",
      "Columns of the predictions file = ['js_ref_id' 'lat' 'lon' 'pred_js' 'cmaq_ins_pred' 'pred_cmaq_outs'\n",
      " 'pred_gs' 'pred_caces' 'pred_av']\n",
      "Shape of the predictions file = (56090, 9)\n",
      "After sampling...\n",
      "Shape of the predictions file = (56090, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, models_on_train, start_date = get_training_data_multi_time(trainDaily, 'obs_pm25', baseModelSet)\n",
    "\n",
    "locations, predictions = get_prediction_data(predDaily, baseModelSet,  1, 0, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit BNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n"
     ]
    }
   ],
   "source": [
    "W_map, w0_map, COV, rr, LL = fit_bne(X_train, y_train, models_on_train, D=500, h=20.0, ht=20.0, sigmasq=-1.0, lmbda=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, model_avg, bias, bne_pred = get_predictions(locations, predictions, W_map, w0_map, rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/_j5q9pcj2cd8q5sfhgxb4qv40000gn/T/ipykernel_29956/2731505044.py:10: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  W_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "bneout = get_uncertainty(locations, predictions, W_map, w0_map, COV, rr, mc_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bne_out.to_csv(RESULTS_DIR +'/bne_test.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BNE TFP Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
