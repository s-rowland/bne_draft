{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0m8xce2x8ZC",
    "outputId": "817c403f-83a9-4401-9c9b-68c2439f35ff"
   },
   "outputs": [],
   "source": [
    "import os as os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import cholesky, cho_solve\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from sklearn.neighbors import BallTree\n",
    "import rff\n",
    "import rff_time\n",
    "import datetime\n",
    "import random as rd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZSLbKXbx1ML"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "ZLDm7Mj5yWLD"
   },
   "outputs": [],
   "source": [
    "def get_training_data_one_time(filePath,obsVar = 'obs_pm25', baseModels = ['av', 'gs','cm', 'js', 'cc']):\n",
    "\t\"\"\"\n",
    "\tReads the training file YEARLY_TRAINING_FILE and returns the training data along with model predictions on training data.\n",
    "\t\"\"\"\n",
    "\t# create the column names \n",
    "\tfor i in range(0,len(baseModels)):\n",
    "\t\tbaseModels[i] = 'pred_' + baseModels[i]\n",
    "\t# Read the training data file using pandas into a DataFrame.\n",
    "    # training data for just one year\n",
    "\ttraining_data = pd.read_csv(filePath)\n",
    "\tprint('Columns of the training file = {}'.format(training_data.columns.values))\n",
    "\tprint('Shape of the training file = {}'.format(training_data.shape))\n",
    "\t# Shuffle the data row-wise\n",
    "\ttraining_data = training_data.sample(frac=1.0)\n",
    "\t# (n, 2) numpy array.\n",
    "    # we do not have time because this is annual data\n",
    "\tX_train = training_data[['lat', 'lon']].values\n",
    "\t# (n, 1) numpy array.\n",
    "\ty_train = training_data[[obsVar]].values\n",
    " # assert checks whether a statement is true and returns an message if it is false\n",
    "\tassert X_train.shape[0] == y_train.shape[0], \"The number of rows in X_train, {0} and y_train don't match {1}\".format(X_train.shape[0], y_train.shape[0])\n",
    "\t# Models' predictions on training data, as a (n, 3).\n",
    "    # we will need to adjust this if we have other predictors\n",
    "\tmodels_on_train = training_data[baseModels].values\n",
    "\tassert models_on_train.shape[0] == X_train.shape[0], \"The number of rows in models_on_train, {0} and X_train don't match {1}\".format(models_on_train.shape[0], X_train.shape[0])\n",
    "\treturn X_train, y_train, models_on_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_multi_time(filePath, obsVar = 'obs_pm2_5', baseModels = ['av_pred', 'gs_pred','cmaq_outs_pred', 'js_pred', 'caces_pred']):\n",
    "\t\"\"\"\n",
    "\tReads the training file TRAINING_FILE and returns the training data along with model predictions on training data.\n",
    "\t\"\"\"\n",
    "\t# create the column names \n",
    "\tfor i in range(0,len(baseModels)):\n",
    "\t\tbaseModels[i] = 'pred_' + baseModels[i]\n",
    "\t# Read the training data file using pandas into a DataFrame.\n",
    "\tdtype_dict = {'year':str, 'month':str, 'day':str} # Read these as string, and others as default\n",
    "\ttraining_data = pd.read_csv(filePath, dtype = dtype_dict)\n",
    "\tprint('Columns of the training file = {}'.format(training_data.columns.values))\n",
    "\tprint('Shape of the training file = {}'.format(training_data.shape))\n",
    "\t# Throw negative or very large outlier values\n",
    "    # data curation should take place elsewhere\n",
    "\ttraining_data = training_data[training_data['obs_pm25'] > 0]\n",
    "\t#training_data = training_data[training_data['obs_pm25'] < 40]\n",
    "\t# Throw non-conterminous states\n",
    "\ttraining_data = training_data[training_data['lat'] < 49.5]\n",
    "\ttraining_data = training_data[training_data['lat'] > 24.5]\n",
    "\ttraining_data = training_data[training_data['lon'] < -66.5]\n",
    "\ttraining_data = training_data[training_data['lon'] > -125]\n",
    "\t# Shuffle the data row-wise\n",
    "\ttraining_data = training_data.sample(frac=1.0)\n",
    "\t# (n, 3) numpy array\n",
    "\tX_train = training_data[['lat', 'lon']].values\n",
    "\tdates = training_data[['year', 'month', 'day']].values\n",
    "\tdates_combined = []\n",
    "\tn = dates.shape[0]\n",
    "\tfor i in range(n):\n",
    "\t\tdates_combined.append('{0}-{1}-{2}'.format(dates[i,0], dates[i,1], dates[i,2]))\n",
    "\tdates_combined = np.array(dates_combined)\n",
    "\tstart_date = sorted(dates_combined)[0]\n",
    "\t#date_start = np.datetime64(start_date)\n",
    "\tdate_start = np.datetime64('2010-01-01')\n",
    " # dates_delta is the julian day\n",
    "\tdates_julian = np.zeros(shape=(n,))\n",
    "\tfor i in range(n):\n",
    "\t\tdates_julian[i] = (np.datetime64(dates_combined[i]) - date_start).astype(int)\n",
    "\t\tdates_julian[i] = dates_julian[i] #% 365 + 10 * np.floor(dates_julian[i] / 365)\n",
    "\tX_train = np.column_stack((X_train, dates_julian))\n",
    "\t# (n, 1) numpy array.\n",
    "\ty_train = training_data[[obsVar]].values\n",
    "\tassert X_train.shape[0] == y_train.shape[0], \"The number of rows in X_train, {0} and y_train don't match {1}\".format(X_train.shape[0], y_train.shape[0])\n",
    "\t# Models' predictions on training data, as a (n, L) for L=6.\n",
    "\tmodels_on_train = training_data[baseModels].values\n",
    "\tassert models_on_train.shape[0] == X_train.shape[0], \"The number of rows in models_on_train, {0} and X_train don't match {1}\".format(models_on_train.shape[0], X_train.shape[0])\n",
    "\tnp.save(RESULTS_DIR + '/X_train.npy', X_train) # Read as X_train = np.load(RESULTS_DIR + '/X_train.npy')\n",
    "\tnp.save(RESULTS_DIR + '/y_train.npy', y_train)\n",
    "\tnp.save(RESULTS_DIR + '/models_on_train.npy', models_on_train)\n",
    "\treturn X_train, y_train, models_on_train, start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_multi_year(filePath, obsVar = 'obs_pm25', baseModels = ['av', 'gs','cm', 'js', 'cc']):\n",
    "\t\"\"\"\n",
    "\tReads the training file TRAINING_FILE and returns the training data along with model predictions on training data.\n",
    "\t\"\"\"\n",
    "\t# create the column names \n",
    "\tpred_names = list(baseModels)\n",
    "\tfor i in range(0,len(baseModels)):\n",
    "\t\tpred_names[i] = 'pred_' + baseModels[i]\n",
    "\t# Read the training data file using pandas into a DataFrame.\n",
    "\tdtype_dict = {'year':str, 'month':str, 'day':str} # Read these as string, and others as default\n",
    "\ttraining_data = pd.read_csv(filePath, dtype = dtype_dict)\n",
    "\tprint('Columns of the training file = {}'.format(training_data.columns.values))\n",
    "\tprint('Shape of the training file = {}'.format(training_data.shape))\n",
    "\t# Throw negative or very large outlier values\n",
    "    # data curation should take place elsewhere\n",
    "\ttraining_data = training_data[training_data['obs_pm25'] > 0]\n",
    "\t#training_data = training_data[training_data['obs_pm25'] < 40]\n",
    "\t# Throw non-conterminous states\n",
    "\ttraining_data = training_data[training_data['lat'] < 49.5]\n",
    "\ttraining_data = training_data[training_data['lat'] > 24.5]\n",
    "\ttraining_data = training_data[training_data['lon'] < -66.5]\n",
    "\ttraining_data = training_data[training_data['lon'] > -125]\n",
    "\t# Shuffle the data row-wise\n",
    "\ttraining_data = training_data.sample(frac=1.0)\n",
    "\t# (n, 3) numpy array\n",
    "\tX_train = training_data[['lat', 'lon']].values\n",
    "\tdates = training_data[['year']].values\n",
    "\tX_train = np.column_stack((X_train, dates))\n",
    "\t# (n, 1) numpy array.\n",
    "\ty_train = training_data[[obsVar]].values\n",
    "\tassert X_train.shape[0] == y_train.shape[0], \"The number of rows in X_train, {0} and y_train don't match {1}\".format(X_train.shape[0], y_train.shape[0])\n",
    "\t# Models' predictions on training data, as a (n, L) for L=6.\n",
    "\tmodels_on_train = training_data[pred_names].values\n",
    "\tassert models_on_train.shape[0] == X_train.shape[0], \"The number of rows in models_on_train, {0} and X_train don't match {1}\".format(models_on_train.shape[0], X_train.shape[0])\n",
    "\tnp.save(RESULTS_DIR + '/X_train.npy', X_train) # Read as X_train = np.load(RESULTS_DIR + '/X_train.npy')\n",
    "\tnp.save(RESULTS_DIR + '/y_train.npy', y_train)\n",
    "\tnp.save(RESULTS_DIR + '/models_on_train.npy', models_on_train)\n",
    "\treturn X_train, y_train, models_on_train, start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_data(filePath, baseModels = ['av', 'gs','cm', 'js', 'cc'], sampleFrac = 1, timeStamp = 0, start_date = '2010-01-01', pred_date='2010-01-01'):\n",
    "\t\"\"\"\n",
    "\tReads the prediction file YEARLY_PREDICTION_FILE and returns\n",
    "\tlocations : (m, 2) numpy array\n",
    "\tpredictions : (m, L) numpy array\n",
    "\t\"\"\"\n",
    "\t# create the column names \n",
    "\tpred_names = list(baseModels)\n",
    "\tfor i in range(0,len(baseModels)):\n",
    "\t\tpred_names[i] = 'pred_' + baseModels[i]\n",
    "\tpred_data = pd.read_csv(filePath)\n",
    "\tif timeStamp != 0:\n",
    "\t\tpred_data = pred_data[pred_data['time'] == timeStamp]\n",
    "\tprint('Columns of the predictions file = {}'.format(pred_data.columns.values))\n",
    "\tprint('Shape of the predictions file = {}'.format(pred_data.shape))\n",
    "\tprint('After sampling...')\n",
    "\tpred_data = pred_data.sample(frac=sampleFrac)\n",
    "\tprint('Shape of the predictions file = {}'.format(pred_data.shape))\n",
    "\tlocations = pred_data[['lat', 'lon', 'day_index']].values\n",
    "\t#date_start = np.datetime64(start_date)\n",
    "\t#date_pred = np.datetime64(pred_date)\n",
    "\t#date_pred_num = date_pred - date_start\n",
    "\t#dates_delta = [date_pred_num] * locations.shape[0]\n",
    "\t#dates_delta = pd.to_timedelta(dates_delta).days\n",
    "\t#locations = np.column_stack((locations, dates_delta))\n",
    "\tpredictions = pred_data[pred_names].values\n",
    "\treturn locations, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_data_annual(filePath, baseModels = ['av', 'gs','cm', 'js', 'cc'], sampleFrac = 1, timeStamp = 0, start_date = '2010-01-01', pred_date='2010-01-01'):\n",
    "\t\"\"\"\n",
    "\tReads the prediction file YEARLY_PREDICTION_FILE and returns\n",
    "\tlocations : (m, 2) numpy array\n",
    "\tpredictions : (m, L) numpy array\n",
    "\t\"\"\"\n",
    "\t# create the column names \n",
    "\tpred_names = list(baseModels)\n",
    "\tfor i in range(0,len(baseModels)):\n",
    "\t\tpred_names[i] = 'pred_' + baseModels[i]\n",
    "\tpred_data = pd.read_csv(filePath)\n",
    "\tprint('Columns of the predictions file = {}'.format(pred_data.columns.values))\n",
    "\tprint('Shape of the predictions file = {}'.format(pred_data.shape))\n",
    "\tprint('After sampling...')\n",
    "\tpred_data = pred_data.sample(frac=sampleFrac)\n",
    "\tprint('Shape of the predictions file = {}'.format(pred_data.shape))\n",
    "\tlocations = pred_data[['lat', 'lon', 'time']].values\n",
    "\t#date_start = np.datetime64(start_date)\n",
    "\t#date_pred = np.datetime64(pred_date)\n",
    "\t#date_pred_num = date_pred - date_start\n",
    "\t#dates_delta = [date_pred_num] * locations.shape[0]\n",
    "\t#dates_delta = pd.to_timedelta(dates_delta).days\n",
    "\t#locations = np.column_stack((locations, dates_delta))\n",
    "\tpredictions = pred_data[pred_names].values\n",
    "\treturn locations, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hessian(Z_train, models_on_train, C, model_avg, error, sigmasq, lmbda=0.1):\n",
    "\t\"\"\"\n",
    "\tComputes the D(L+1) x D(L+1) Hessian.\n",
    "\tFor L = 3, it looks like as follows:\n",
    "\t-----------------\n",
    "\t|w_0|0,1|0,2|0,3|\n",
    "\t-----------------\n",
    "\t|1,0|w_1|1,2|1,3|\n",
    "\t-----------------\n",
    "\t|2,0|2,1|w_2|2,3|\n",
    "\t-----------------\n",
    "\t|3,0|3,1|3,2|w_3|\n",
    "\t-----------------\n",
    "\tw_l denotes the DxD block nabla_{w_l}^2\n",
    "\tl,j denotes the DxD block nabla_{w_l} nabla_{w_j}\n",
    "\n",
    "\t# Inputs\n",
    "\tZ_train : (n, D) numpy array containing n projected data points\n",
    "\tmodels_on_train : (n, L) numpy array containing the model predictions on the training inputs\n",
    "\tC : (n, L) numpy array containing probabilitic weights of each model. Each row sums to 1\n",
    "\tmodel_avg = (n, 1) numpy array containing the models' average prediction obained from models_on_train and C\n",
    "\terror : (n, 1) numpy array containing the difference between the true values and ensemble prediction\n",
    "\tsigmasq : variance of the noise\n",
    "\tlmbda : the prior on the weights is N(0, 1/lmbda I_D)\n",
    "\n",
    "\t# Output\n",
    "\t\"\"\"\n",
    "\tn, D = Z_train.shape\n",
    "\tL = models_on_train.shape[1]\n",
    "\t# The Hessian initialized to 0.\n",
    "\tH = np.zeros(shape=(D*(L+1), D*(L+1)))\n",
    "\t# nabla_{w_0}^2 block\n",
    "\tnabla_0_0 = - (1/sigmasq) * np.matmul(Z_train.T, Z_train) - lmbda * np.identity(D)\n",
    "\t# Put nabla_0_0 in the top DxD block of H\n",
    "\tH[:D, :D]= nabla_0_0\n",
    "\t# (n, L) matrix storing the difference between a model prediction and models' average\n",
    "\tmodels_diff = models_on_train - model_avg\n",
    "\t# nabla_{w_l} nabla_{w_0} blocks\n",
    "\tfor l in range(L):\n",
    "\t\tnabla_l_0 = - (1/sigmasq) * np.matmul(Z_train.T, np.multiply(Z_train, np.multiply(C[:, l, None], models_diff[:, l, None])))\n",
    "\t\t# Put nabla_l_0 in the corresponding positions\n",
    "\t\tH[(l+1)*D:(l+2)*D, :D] = nabla_l_0\n",
    "\t\tH[:D, (l+1)*D:(l+2)*D] = nabla_l_0\n",
    "\t# nabla_{w_l}^2 blocks\n",
    "\tfor l in range(L):\n",
    "\t\tfactor = np.multiply(C[:, l, None], np.multiply(models_diff[:, l, None], np.multiply(C[:, l, None], models_diff[:, l, None]) + 2 * np.multiply(C[:, l, None], error) - error))\n",
    "\t\tnabla_l_l = -lmbda * np.identity(D) - (1/sigmasq) * np.matmul(Z_train.T, np.multiply(Z_train, factor))\n",
    "\t\t# Put nabla_l_l in H\n",
    "\t\tH[(l+1)*D:(l+2)*D, (l+1)*D:(l+2)*D] = nabla_l_l\n",
    "\t# nabla_{w_j} nabla_{w_l} blocks\n",
    "\tfor j in range(L):\n",
    "\t\tfor l in range(j+1, L):\n",
    "\t\t\tfactor = np.multiply(C[:, j, None], np.multiply(C[:, l, None], np.multiply(models_diff[:, j, None], models_diff[:, l, None]) + np.multiply(error, models_diff[:, l, None]) + np.multiply(error, models_diff[:, j, None])))\n",
    "\t\t\tnabla_j_l = - (1/sigmasq) * np.matmul(Z_train.T, np.multiply(Z_train, factor))\n",
    "\t\t\t# Put nabla_j_l in H\n",
    "\t\t\tH[(j+1)*D:(j+2)*D, (l+1)*D:(l+2)*D] = nabla_j_l\n",
    "\t\t\tH[(l+1)*D:(l+2)*D, (j+1)*D:(j+2)*D] = nabla_j_l\n",
    "\treturn H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bne(X_train, y_train, models_on_train, D=500, h=20.0, ht=20.0, sigmasq=-1.0, lmbda=1.0, lmbda0=1.0):\n",
    "\t\"\"\"\n",
    "\tTrains the RFF approximation of BNE using gradient ascent.\n",
    "\n",
    "\t# Inputs\n",
    "\tX_train : (n, d) numpy array containing the \"raw\" data inputs\n",
    "\ty_train : (n, 1) numpy array containing the monitor observations\n",
    "\tmodels_on_train : (n, L) numpy array containing the model predictions on the training inputs\n",
    "\tD : Number of RFF features\n",
    "\th : length scale for the Gaussian kernel exp(-|x-y|^2 / (2 * h)). The corresponding spectral measure is N(0, 1/h I_d)\n",
    "\tht : length scale for time\n",
    "\tsigmasq : variance of the noise\n",
    "\tlmbda : the prior on the weights is N(0, 1/lmbda I_D)\n",
    "\t\"\"\"\n",
    "\tn, d = X_train.shape\n",
    "\tL = models_on_train.shape[1]\n",
    "    # change this line if doing spatial-only model\n",
    "\tnp.random.seed(0)\n",
    "\trr = rff_time.RFF_TIME(X_train, D=D, h=h, ht=ht)\n",
    "\t# SNR of 8\n",
    "\tif sigmasq < 0:\n",
    "\t\tsigmasq = np.var(y_train) / 8\n",
    "\t# (n, D) numpy array containing the \"projected\" data inputs\n",
    "\tZ_train = rr.get_Z(X_train)\n",
    "\t# Initialize the weights\n",
    "\tnp.random.seed(0)\n",
    "\tw0 = np.random.normal(size=(D, 1))\n",
    "\tnp.random.seed(0)\n",
    "\tW = np.random.normal(size=(D, L))\n",
    "\t# Hyperparameters for the gradient ascent\n",
    "\tMAX_ITER = 2000\n",
    "\tMAX_STEP_SIZE = 0.1\n",
    "\tBATCH_SIZE = 5000\n",
    "\tSPLITS = int(np.ceil(n / BATCH_SIZE))\n",
    "\tLL = []\n",
    "\tfor it in range(1, MAX_ITER+1):\n",
    "\t\tstep_size = MAX_STEP_SIZE / np.sqrt(it)\n",
    "\t\tfor b in range(SPLITS):\n",
    "\t\t\t# Construct (BATCH_SIZE, D) matrix used for training\n",
    "\t\t\tZ = Z_train[b*BATCH_SIZE : min((b+1)*BATCH_SIZE, n)]\n",
    "\t\t\t# (BATCH_SIZE, 1)\n",
    "\t\t\ty = y_train[b*BATCH_SIZE : min((b+1)*BATCH_SIZE, n)]\n",
    "\t\t\t# (BATCH_SIZE, L)\n",
    "\t\t\tmodels = models_on_train[b*BATCH_SIZE : min((b+1)*BATCH_SIZE, n)]\n",
    "\t\t\t### Update W ###\n",
    "\t\t\t# Fourier GP approximation of shape (BATCH_SIZE, L)\n",
    "\t\t\tG = np.matmul(Z, W)\n",
    "\t\t\t# Each row sums to 1. (BATCH_SIZE, L) shape\n",
    "\t\t\tC = softmax(G, axis=1)\n",
    "\t\t\t# At each point get the model average. Shape (BATCH_SIZE, 1)\n",
    "\t\t\tmodel_avg = np.sum(np.multiply(C, models), axis=1, keepdims=True)\n",
    "\t\t\t# Bias term. Shape (BATCH_SIZE, 1)\n",
    "\t\t\tbias = np.matmul(Z, w0)\n",
    "\t\t\t# Error. (BATCH_SIZE, 1)\n",
    "\t\t\terror = y - model_avg - bias\n",
    "\t\t\t# Compute gradient\n",
    "\t\t\tgradient_W = (1/sigmasq) * np.matmul(Z.T, np.multiply(C, np.multiply(np.tile(error, L), models - model_avg))) - lmbda * W\n",
    "\t\t\tW = W + step_size * gradient_W\n",
    "\t\t\t### Update w0 ###\n",
    "\t\t\t# Fourier GP approximation of shape (BATCH_SIZE, L)\n",
    "\t\t\tG = np.matmul(Z, W)\n",
    "\t\t\t# Each row sums to 1. (BATCH_SIZE, L) shape\n",
    "\t\t\tC = softmax(G, axis=1)\n",
    "\t\t\t# At each point get the model average. Shape (BATCH_SIZE, 1)\n",
    "\t\t\tmodel_avg = np.sum(np.multiply(C, models), axis=1, keepdims=True)\n",
    "\t\t\t# Error. (BATCH_SIZE, 1)\n",
    "\t\t\terror = y - model_avg - bias\n",
    "\t\t\t# Compute gradient\n",
    "\t\t\tgradient_w0 = (1/sigmasq) * np.matmul(Z.T, error) - lmbda0 * w0\n",
    "\t\t\tw0 = w0 + step_size * gradient_w0\n",
    "\t\t# Compute log joint likelihood\n",
    "\t\tif it % 10 == 0:\n",
    "\t\t\tprint('Iteration: {}'.format(it))\n",
    "\t\t\tG = np.matmul(Z_train, W)\n",
    "\t\t\tC = softmax(G, axis=1) # rescale the weights \n",
    "\t\t\tmodel_avg = np.sum(np.multiply(C, models_on_train), axis=1, keepdims=True) # compute model average\n",
    "\t\t\tbias = np.matmul(Z_train, w0) # the key thing is that we only update the bias every 10 iterations\n",
    "\t\t\terror = y_train - model_avg - bias\n",
    "\t\t\tLL.append((-1/(2*sigmasq)) * np.sum(error**2) - (lmbda/2) * np.sum(W**2) - (lmbda0/2) * np.sum(w0**2))\n",
    "\t# Compute the covariance matrix of the Laplace approximation\n",
    "\tG = np.matmul(Z_train, W)\n",
    "\tC = softmax(G, axis=1)\n",
    "\tmodel_avg = np.sum(np.multiply(C, models_on_train), axis=1, keepdims=True)\n",
    "\tbias = np.matmul(Z_train, w0)\n",
    "\terror = y_train - model_avg - bias\n",
    "\tH = compute_hessian(Z_train, models_on_train, C, model_avg, error, sigmasq, lmbda)\n",
    "\t# Covariance matrix is the inverse of the negative of the Hessian evaluated at MAP\n",
    "\tCOV = np.linalg.inv(-H)\n",
    "\treturn W, w0, COV, rr, LL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(locations, predictions, W, w0, rr):\n",
    "\t\"\"\"\n",
    "\tGet BNE predictions at locations.\n",
    "\n",
    "\t# Inputs\n",
    "\tlocations : (m, 3) numpy array # location in space and time\n",
    "\tpredictions : (m, L) numpy array\n",
    "\tW : (D, L) numpy array\n",
    "\tw0 : (D, 1) numpy array\n",
    "\trr : RFF object\n",
    "\tm : number of points to predict on\n",
    "\t# Returns\n",
    "\t\"\"\"\n",
    "\t# (m, D) shape\n",
    "\tZ_pred = rr.get_Z(locations)\n",
    "\t# GP Fourier approximation of shape (m, L)\n",
    "\tG = np.matmul(Z_pred, W)\n",
    "\t# Each row sums to 1. (m, L) shape\n",
    "\tC = softmax(G, axis=1)\n",
    "\t# At each location in locations, get model average. Shape (m, 1)\n",
    "\tmodel_avg = np.sum(np.multiply(C, predictions), axis=1, keepdims=True)\n",
    "\t# Bias term. Shape (m, 1)\n",
    "\tbias = np.matmul(Z_pred, w0)\n",
    "\tbne_pred = model_avg + bias\n",
    "    # C is the properly-sclaed weighted, model_avg is the core ensemble component \n",
    "    # bias is the systematic bias term, and bne_pred is the actual predictions\n",
    "\treturn C, model_avg, bias, bne_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty(locations, predictions, W_map, w0_map, COV, rr, mc_samples=100, baseModels = ['av', 'gs','cm', 'js', 'cc']):\n",
    "\n",
    "\n",
    "\t# create the column names \n",
    "\tw_mean_names = list(baseModels)\n",
    "\tw_sd_names = list(baseModels) \n",
    "\tfor i in range(0,len(baseModels)):\n",
    "\t\tw_mean_names[i] = 'w_mean_' + baseModels[i]\n",
    "\t\tw_sd_names[i] = 'w_sd_' + baseModels[i]\n",
    "\tm, L = predictions.shape\n",
    "\tD = W_map.shape[0]\n",
    "\tW_samples = np.empty(shape=(mc_samples, m, L))\n",
    "\tmodel_avg_samples = np.empty(shape=(mc_samples, m))\n",
    "\tw0_samples = np.empty(shape=(mc_samples, m))\n",
    "\tbne_pred_samples = np.empty(shape=(mc_samples, m))\n",
    "\t# (mc_samples, D*(L+1))\n",
    "\tnp.random.seed(0)\n",
    "\tW_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n",
    "\tfor s in range(mc_samples):\n",
    "\t\tif s % 50 == 0:\n",
    "\t\t\tprint(s)     \n",
    "\t\tW = np.reshape(W_w0_samples[s, :D*L], newshape=(D, L))\n",
    "\t\tw0 = np.reshape(W_w0_samples[s, D*L:], newshape=(D, 1))\n",
    "\t\tW_samp, model_avg_samp, w0_samp, bne_pred_samp = get_predictions(locations, predictions, W, w0, rr)  \n",
    "\t\tfor l in range(L):\n",
    "\t\t\tW_samples[s, :, l] = W_samp[:, l] \n",
    "\t\tmodel_avg_samples[s, :] = model_avg_samp[:, 0] \n",
    "\t\tw0_samples[s, :] = w0_samp[:, 0] \n",
    "\t\tbne_pred_samples[s, :] = bne_pred_samp[:, 0] \n",
    "\t# calculate summary metrics \n",
    "\tW_mean = np.mean(W_samples, axis=(0))\n",
    "\tW_std = np.sqrt(np.var(W_samples, axis=0))\n",
    "\tmodel_avg_mean = np.mean(model_avg_samples, axis=0)\n",
    "\tmodel_avg_std = np.sqrt(np.var(model_avg_samples, axis=0))\n",
    "\tw0_mean = np.mean(w0_samples, axis=0)\n",
    "\tw0_std = np.sqrt(np.var(w0_samples, axis=0))\n",
    "\tbne_preds_mean = np.mean(bne_pred_samples, axis=0)\n",
    "\tbne_preds_std = np.sqrt(np.var(bne_pred_samples, axis=0))\n",
    "\t# combine into a nice dataframe\n",
    "\tbne_out = np.column_stack((model_avg_mean, model_avg_std, w0_mean, w0_std, bne_preds_mean, bne_preds_std))\n",
    "\tbne_out = np.column_stack((locations, W_mean, W_std, bne_out))\n",
    "\tcolNames = ['lat', 'lon', 'time']+w_mean_names+w_sd_names+['ens_mean', 'ens_sd','res_mean', 'res_sd', 'pred_mean', 'pred_sd']\n",
    "\tbne_out_df = pd.DataFrame(bne_out, columns = colNames)\n",
    "#C, model_avg, bias, bne_pred\n",
    "\n",
    "# save to results directory\n",
    "\t\n",
    "\t# Compute the mean also\n",
    "\treturn bne_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cv(X_train, y_train, models_on_train, D=500, h=10.0, ht=10.0, lmbda=1.0, splits=20):\n",
    "\tprint(datetime.datetime.now())\n",
    "\tn, d = X_train.shape\n",
    "\tchunk = int(n/splits)\n",
    "\tmae = []\n",
    "\trmse = []\n",
    "\tcoverage = []\n",
    "\tfor split in range(splits):\n",
    "\t\tprint('Split: {0}  at {1}'.format(split, datetime.datetime.now()))\n",
    "# we can just add filter statements, and a column for leaveOutfold\n",
    "\t\tflag = np.full((n,), True)\n",
    "\t\tflag[split*chunk : min((split+1)*chunk, n)] = False\n",
    "\t\tX_train1 = X_train[flag]\n",
    "\t\tX_train2 = X_train[np.invert(flag)]\n",
    "\t\ty_train1 = y_train[flag]\n",
    "\t\ty_train2 = y_train[np.invert(flag)]\n",
    "\t\tmodels_on_train1 = models_on_train[flag]\n",
    "\t\tmodels_on_train2 = models_on_train[np.invert(flag)]\n",
    "\t\tprint('D = {}'.format(D))\n",
    "\t\tprint('h = {}'.format(h))\n",
    "\t\tprint('ht = {}'.format(ht))\n",
    "\t\tprint('lmbda = {}'.format(lmbda))\n",
    "\t\tW, w0, COV, rr, LL = fit_bne(X_train1, y_train1, models_on_train1, D=D, h=h, ht=ht, sigmasq=-1.0, lmbda=lmbda)\n",
    "\t\tbne_pred, model_avg, bias = get_predictions(X_train2, models_on_train2, W, w0, rr)\n",
    "\t\t# Errors\n",
    "\t\tmae.append(np.sum(np.abs(y_train2 - bne_pred))/y_train2.shape[0])\n",
    "\t\trmse.append(np.sqrt(np.sum((y_train2 - bne_pred)**2)/y_train2.shape[0]))\n",
    "\t\t# Coverage\n",
    "\t\temp_std = get_uncertainty(X_train2, models_on_train2, W, w0, COV, rr, mc_samples=100)\n",
    "\t\tcovered = 0\n",
    "\t\tfor i in range(y_train2.shape[0]):\n",
    "\t\t\tif y_train2[i,0] >= bne_pred[i,0] - 2 * emp_std[i] and y_train2[i,0] <= bne_pred[i,0] + 2 * emp_std[i]:\n",
    "\t\t\t\tcovered += 1\n",
    "\t\tcoverage.append( covered / y_train2.shape[0])\n",
    "\tprint(datetime.datetime.now())\n",
    "\treturn mae, rmse, coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit BNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ABs_TxXf1ke"
   },
   "source": [
    "## Name Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "m1IShAoGxsRk"
   },
   "outputs": [],
   "source": [
    "# combined training dataset location\n",
    "os.chdir('/Users/sebastianrowland/Desktop/BNE/bne_draft')\n",
    "trainDailyspt = 'BNE_inputs/training_datasets/yearly_daily/training_avcmjsme_2010_all.csv'\n",
    "predDailyspt =  'BNE_inputs/prediction_datasets/individual_daily/prediction_avcmjsme_2010_1_all.csv'\n",
    "\n",
    "#trainAnnualsp = 'training_datasets/training_annual_2010_all.csv'\n",
    "#predAnnualsp = \"prediction_datasets/predictions_annual_2010_all.csv\"\n",
    "\n",
    "#trainDaily = 'training_datasets/dailyTrainingData_2010-2015.csv'\n",
    "#predDaily = \"prediction_datasets/daily_pred_2010-01-01_small.csv\"\n",
    "\n",
    "RESULTS_DIR = \"./outputs/py_bne_test\"\n",
    "\n",
    "baseModelSet = ['pred_av', 'pred_cm', 'pred_js', 'pred_me']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of the training file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'obs_pm25' 'pred_av'\n",
      " 'pred_cm' 'pred_js' 'pred_me' 'ref_id']\n",
      "Shape of the training file = (111373, 12)\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n"
     ]
    }
   ],
   "source": [
    "#X_train, y_train, models_on_train, start_date = get_training_data_multi_time(trainDaily, 'obs_pm25', baseModelSet)\n",
    "#locations, predictions = get_prediction_data(predDaily, baseModelSet,  1, 0, start_date)\n",
    "X_train, y_train, models_on_train, start_date = get_training_data_multi_time(trainDailyspt, 'obs_pm25', baseModelSet)\n",
    "locations, predictions = get_prediction_data(predDailyspt, baseModelSet,  1, 0, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit BNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n",
      "Iteration: 110\n",
      "Iteration: 120\n",
      "Iteration: 130\n",
      "Iteration: 140\n",
      "Iteration: 150\n",
      "Iteration: 160\n",
      "Iteration: 170\n",
      "Iteration: 180\n",
      "Iteration: 190\n",
      "Iteration: 200\n",
      "Iteration: 210\n",
      "Iteration: 220\n",
      "Iteration: 230\n",
      "Iteration: 240\n",
      "Iteration: 250\n",
      "Iteration: 260\n",
      "Iteration: 270\n",
      "Iteration: 280\n",
      "Iteration: 290\n",
      "Iteration: 300\n",
      "Iteration: 310\n",
      "Iteration: 320\n",
      "Iteration: 330\n",
      "Iteration: 340\n",
      "Iteration: 350\n",
      "Iteration: 360\n",
      "Iteration: 370\n",
      "Iteration: 380\n",
      "Iteration: 390\n",
      "Iteration: 400\n",
      "Iteration: 410\n",
      "Iteration: 420\n",
      "Iteration: 430\n",
      "Iteration: 440\n",
      "Iteration: 450\n",
      "Iteration: 460\n",
      "Iteration: 470\n",
      "Iteration: 480\n",
      "Iteration: 490\n",
      "Iteration: 500\n",
      "Iteration: 510\n",
      "Iteration: 520\n",
      "Iteration: 530\n",
      "Iteration: 540\n",
      "Iteration: 550\n",
      "Iteration: 560\n",
      "Iteration: 570\n",
      "Iteration: 580\n",
      "Iteration: 590\n",
      "Iteration: 600\n",
      "Iteration: 610\n",
      "Iteration: 620\n",
      "Iteration: 630\n",
      "Iteration: 640\n",
      "Iteration: 650\n",
      "Iteration: 660\n",
      "Iteration: 670\n",
      "Iteration: 680\n",
      "Iteration: 690\n",
      "Iteration: 700\n",
      "Iteration: 710\n",
      "Iteration: 720\n",
      "Iteration: 730\n",
      "Iteration: 740\n",
      "Iteration: 750\n",
      "Iteration: 760\n",
      "Iteration: 770\n",
      "Iteration: 780\n",
      "Iteration: 790\n",
      "Iteration: 800\n",
      "Iteration: 810\n",
      "Iteration: 820\n",
      "Iteration: 830\n",
      "Iteration: 840\n",
      "Iteration: 850\n",
      "Iteration: 860\n",
      "Iteration: 870\n",
      "Iteration: 880\n",
      "Iteration: 890\n",
      "Iteration: 900\n",
      "Iteration: 910\n",
      "Iteration: 920\n",
      "Iteration: 930\n",
      "Iteration: 940\n",
      "Iteration: 950\n",
      "Iteration: 960\n",
      "Iteration: 970\n",
      "Iteration: 980\n",
      "Iteration: 990\n",
      "Iteration: 1000\n",
      "Iteration: 1010\n",
      "Iteration: 1020\n",
      "Iteration: 1030\n",
      "Iteration: 1040\n",
      "Iteration: 1050\n",
      "Iteration: 1060\n",
      "Iteration: 1070\n",
      "Iteration: 1080\n",
      "Iteration: 1090\n",
      "Iteration: 1100\n",
      "Iteration: 1110\n",
      "Iteration: 1120\n",
      "Iteration: 1130\n",
      "Iteration: 1140\n",
      "Iteration: 1150\n",
      "Iteration: 1160\n",
      "Iteration: 1170\n",
      "Iteration: 1180\n",
      "Iteration: 1190\n",
      "Iteration: 1200\n",
      "Iteration: 1210\n",
      "Iteration: 1220\n",
      "Iteration: 1230\n",
      "Iteration: 1240\n",
      "Iteration: 1250\n",
      "Iteration: 1260\n",
      "Iteration: 1270\n",
      "Iteration: 1280\n",
      "Iteration: 1290\n",
      "Iteration: 1300\n",
      "Iteration: 1310\n",
      "Iteration: 1320\n",
      "Iteration: 1330\n",
      "Iteration: 1340\n",
      "Iteration: 1350\n",
      "Iteration: 1360\n",
      "Iteration: 1370\n",
      "Iteration: 1380\n",
      "Iteration: 1390\n",
      "Iteration: 1400\n",
      "Iteration: 1410\n",
      "Iteration: 1420\n",
      "Iteration: 1430\n",
      "Iteration: 1440\n",
      "Iteration: 1450\n",
      "Iteration: 1460\n",
      "Iteration: 1470\n",
      "Iteration: 1480\n",
      "Iteration: 1490\n",
      "Iteration: 1500\n",
      "Iteration: 1510\n",
      "Iteration: 1520\n",
      "Iteration: 1530\n",
      "Iteration: 1540\n",
      "Iteration: 1550\n",
      "Iteration: 1560\n",
      "Iteration: 1570\n",
      "Iteration: 1580\n",
      "Iteration: 1590\n",
      "Iteration: 1600\n",
      "Iteration: 1610\n",
      "Iteration: 1620\n",
      "Iteration: 1630\n",
      "Iteration: 1640\n",
      "Iteration: 1650\n",
      "Iteration: 1660\n",
      "Iteration: 1670\n",
      "Iteration: 1680\n",
      "Iteration: 1690\n",
      "Iteration: 1700\n",
      "Iteration: 1710\n",
      "Iteration: 1720\n",
      "Iteration: 1730\n",
      "Iteration: 1740\n",
      "Iteration: 1750\n",
      "Iteration: 1760\n",
      "Iteration: 1770\n",
      "Iteration: 1780\n",
      "Iteration: 1790\n",
      "Iteration: 1800\n",
      "Iteration: 1810\n",
      "Iteration: 1820\n",
      "Iteration: 1830\n",
      "Iteration: 1840\n",
      "Iteration: 1850\n",
      "Iteration: 1860\n",
      "Iteration: 1870\n",
      "Iteration: 1880\n",
      "Iteration: 1890\n",
      "Iteration: 1900\n",
      "Iteration: 1910\n",
      "Iteration: 1920\n",
      "Iteration: 1930\n",
      "Iteration: 1940\n",
      "Iteration: 1950\n",
      "Iteration: 1960\n",
      "Iteration: 1970\n",
      "Iteration: 1980\n",
      "Iteration: 1990\n",
      "Iteration: 2000\n"
     ]
    }
   ],
   "source": [
    "W_map, w0_map, COV, rr, LL = fit_bne(X_train, y_train, models_on_train, D=500, h=7.0, ht=20.0, sigmasq=-1.0, lmbda=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train, models_on_train, start_date = get_training_data_multi_time(trainDaily, 'obs_pm25', baseModelSet)\n",
    "#locations, predictions = get_prediction_data(predDaily, baseModelSet,  1, 0, start_date)\n",
    "X_train, y_train, models_on_train, start_date = get_training_data_multi_time(trainDailyspt, 'obs_pm25', baseModelSet)\n",
    "locations, predictions = get_prediction_data(predDailyspt, baseModelSet,  1, 0, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, model_avg, bias, bne_pred = get_predictions(locations, predictions, W_map, w0_map, rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-7e5e8854380c>:10: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  W_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "bneout = get_uncertainty(locations, predictions, W_map, w0_map, COV, rr, mc_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bneout.to_csv(RESULTS_DIR +'/bne_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  34.85404504,  -75.65717244,    3.        ],\n",
       "       [  34.54545455,  -94.65483488,    3.        ],\n",
       "       [  38.59049208, -109.59624259,    3.        ],\n",
       "       [  42.09341118, -114.28295688,    3.        ],\n",
       "       [  30.46705588,  -81.9590364 ,    3.        ],\n",
       "       [  39.68306922, -106.59599653,    3.        ],\n",
       "       [  38.64053378, -119.75266325,    3.        ],\n",
       "       [  40.18348624,  -92.56404153,    3.        ],\n",
       "       [  39.53294412, -117.28712179,    3.        ],\n",
       "       [  47.12260217,  -95.14146338,    3.        ]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "julianDay = '3'\n",
    "predDailyspt =  'BNE_inputs/prediction_datasets/individual_daily/prediction_avcmjsme_2010_' + julianDay + '_all.csv'\n",
    "locations, predictions = get_prediction_data(predDailyspt, baseModelSet,  1, 0, start_date)\n",
    "locations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_bneout(julianDay):\n",
    "    # string of the prediction dataset for that day\n",
    "\tpredDailyspt =  'BNE_inputs/prediction_datasets/individual_daily/prediction_avcmjsme_2010_' + julianDay + '_all.csv'\n",
    "    # readin the prediction dataset\n",
    "\tlocations, predictions = get_prediction_data(predDailyspt, baseModelSet,  1, 0, start_date)\n",
    "    \n",
    "    # generate main predictions\n",
    "\tC, model_avg, bias, bne_pred = get_predictions(locations, predictions, W_map, w0_map, rr)\n",
    "\t# set number of samples to take \n",
    "\tmc_samples = 100\n",
    "\tm, L = predictions.shape\n",
    "\tD = W_map.shape[0]\n",
    "\tW_samples = np.empty(shape=(mc_samples, m, L))\n",
    "\tmodel_avg_samples = np.empty(shape=(mc_samples, m))\n",
    "\tw0_samples = np.empty(shape=(mc_samples, m))\n",
    "\tbne_pred_samples = np.empty(shape=(mc_samples, m))\n",
    "\t# (mc_samples, D*(L+1))\n",
    "\tnp.random.seed(0)\n",
    "\tW_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n",
    "\tfor s in range(mc_samples):\n",
    "\t\tif s % 50 == 0:\n",
    "\t\t\tprint(s)     \n",
    "\t\tW = np.reshape(W_w0_samples[s, :D*L], newshape=(D, L))\n",
    "\t\tw0 = np.reshape(W_w0_samples[s, D*L:], newshape=(D, 1))\n",
    "\t\tW_samp, model_avg_samp, w0_samp, bne_pred_samp = get_predictions(locations, predictions, W, w0, rr)  \n",
    "\t\tfor l in range(L):\n",
    "\t\t\tW_samples[s, :, l] = W_samp[:, l] \n",
    "\t\tmodel_avg_samples[s, :] = model_avg_samp[:, 0] \n",
    "\t\tw0_samples[s, :] = w0_samp[:, 0] \n",
    "\t\tbne_pred_samples[s, :] = bne_pred_samp[:, 0] \n",
    "\t# calculate summary metrics \n",
    "\tW_mean = np.mean(W_samples, axis=(0))\n",
    "\tW_std = np.sqrt(np.var(W_samples, axis=0))\n",
    "\tmodel_avg_mean = np.mean(model_avg_samples, axis=0)\n",
    "\tmodel_avg_std = np.sqrt(np.var(model_avg_samples, axis=0))\n",
    "\tw0_mean = np.mean(w0_samples, axis=0)\n",
    "\tw0_std = np.sqrt(np.var(w0_samples, axis=0))\n",
    "\tbne_preds_mean = np.mean(bne_pred_samples, axis=0)\n",
    "\tbne_preds_std = np.sqrt(np.var(bne_pred_samples, axis=0))\n",
    "\t# combine into a nice dataframe\n",
    "\tbne_out = np.column_stack((model_avg_mean, model_avg_std, w0_mean, w0_std, bne_preds_mean, bne_preds_std))\n",
    "\tbne_out = np.column_stack((locations, W_mean, W_std, bne_out))\n",
    "\tcolNames = ['lat', 'lon', 'time','w_mean_av', 'w_mean_cm', 'w_mean_js', 'w_mean_me', 'w_sd_av', 'w_sd_cm', 'w_sd_js', 'w_sd_me', 'ens_mean', 'ens_sd','res_mean', 'res_sd', 'pred_mean', 'pred_sd']\n",
    "\tbne_out_df = pd.DataFrame(bne_out, columns = colNames)\n",
    "#C, model_avg, bias, bne_pred\n",
    "\n",
    "# save to results directory\n",
    "\t\n",
    "\t# Compute the mean also\n",
    "\tbne_out_df.to_csv(RESULTS_DIR +'/bne_avcmjsme_2010_' + julianDay + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-129-cdc9adb57782>:19: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  W_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "30\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "31\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "32\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "33\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "34\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "35\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "36\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "37\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "38\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "39\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "40\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "41\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "42\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "43\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "44\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "45\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "46\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "47\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "48\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "49\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "50\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "51\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "52\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "53\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "54\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "55\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "56\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "57\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "58\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n",
      "59\n",
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'pred_av' 'pred_cm'\n",
      " 'pred_js' 'pred_me']\n",
      "Shape of the predictions file = (110405, 10)\n",
      "After sampling...\n",
      "Shape of the predictions file = (110405, 10)\n",
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#numbers = range(60, 365)\n",
    "\n",
    "\n",
    "#sequence_of_numbers = [2, 3, 4]\n",
    "sequence_of_numbers = range(29, 60)\n",
    "#for number in numbers:\n",
    "\n",
    " #  if number % 30 in (0, 15):\n",
    "\n",
    "  #    sequence_of_numbers.append(number)\n",
    "for x in sequence_of_numbers:\n",
    "    print(x)\n",
    "    store_bneout(str(x))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of the predictions file = ['lat' 'lon' 'day_index' 'year' 'month' 'day' 'obs_pm25' 'pred_av'\n",
      " 'pred_cm' 'pred_js' 'pred_me' 'ref_id']\n",
      "Shape of the predictions file = (111373, 12)\n",
      "After sampling...\n",
      "Shape of the predictions file = (111373, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-143-34c822cdcc6f>:20: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  W_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "    # string of the prediction dataset for that day\n",
    "    \n",
    "    \n",
    "predDailyspt =  'BNE_inputs/training_datasets/yearly_daily/training_avcmjsme_2010_all.csv'\n",
    "    # readin the prediction dataset\n",
    "locations, predictions = get_prediction_data(predDailyspt, baseModelSet,  1, 0, start_date)\n",
    "    \n",
    "    # generate main predictions\n",
    "C, model_avg, bias, bne_pred = get_predictions(locations, predictions, W_map, w0_map, rr)\n",
    "\t# set number of samples to take \n",
    "mc_samples = 100\n",
    "m, L = predictions.shape\n",
    "D = W_map.shape[0]\n",
    "W_samples = np.empty(shape=(mc_samples, m, L))\n",
    "model_avg_samples = np.empty(shape=(mc_samples, m))\n",
    "w0_samples = np.empty(shape=(mc_samples, m))\n",
    "bne_pred_samples = np.empty(shape=(mc_samples, m))\n",
    "\t# (mc_samples, D*(L+1))\n",
    "np.random.seed(0)\n",
    "W_w0_samples = np.random.multivariate_normal(np.reshape(np.hstack((W_map, w0_map)), newshape=(D*(L+1), )), COV, size=mc_samples)\n",
    "for s in range(mc_samples):\n",
    "\tif s % 50 == 0:\n",
    "\t\tprint(s)     \n",
    "\tW = np.reshape(W_w0_samples[s, :D*L], newshape=(D, L))\n",
    "\tw0 = np.reshape(W_w0_samples[s, D*L:], newshape=(D, 1))\n",
    "\tW_samp, model_avg_samp, w0_samp, bne_pred_samp = get_predictions(locations, predictions, W, w0, rr)  \n",
    "\tfor l in range(L):\n",
    "\t\tW_samples[s, :, l] = W_samp[:, l] \n",
    "\tmodel_avg_samples[s, :] = model_avg_samp[:, 0] \n",
    "\tw0_samples[s, :] = w0_samp[:, 0] \n",
    "\tbne_pred_samples[s, :] = bne_pred_samp[:, 0] \n",
    "\t# calculate summary metrics \n",
    "W_mean = np.mean(W_samples, axis=(0))\n",
    "W_std = np.sqrt(np.var(W_samples, axis=0))\n",
    "model_avg_mean = np.mean(model_avg_samples, axis=0)\n",
    "model_avg_std = np.sqrt(np.var(model_avg_samples, axis=0))\n",
    "w0_mean = np.mean(w0_samples, axis=0)\n",
    "w0_std = np.sqrt(np.var(w0_samples, axis=0))\n",
    "bne_preds_mean = np.mean(bne_pred_samples, axis=0)\n",
    "bne_preds_std = np.sqrt(np.var(bne_pred_samples, axis=0))\n",
    "# combine into a nice dataframe\n",
    "bne_out = np.column_stack((model_avg_mean, model_avg_std, w0_mean, w0_std, bne_preds_mean, bne_preds_std))\n",
    "bne_out = np.column_stack((locations, W_mean, W_std, bne_out))\n",
    "colNames = ['lat', 'lon', 'time','w_mean_av', 'w_mean_cm', 'w_mean_js', 'w_mean_me', 'w_sd_av', 'w_sd_cm', 'w_sd_js', 'w_sd_me', 'ens_mean', 'ens_sd','res_mean', 'res_sd', 'pred_mean', 'pred_sd']\n",
    "bne_out_df = pd.DataFrame(bne_out, columns = colNames)\n",
    "#C, model_avg, bias, bne_pred\n",
    "\n",
    "# save to results directory\n",
    "\t\n",
    "\t# Compute the mean also\n",
    "bne_out_df.to_csv(RESULTS_DIR +'/bne_avcmjsme_2010_' + 'trainingPoints' + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annual Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of the training file = ['lat' 'lon' 'year' 'obs_pm25' 'pred_av' 'pred_gs' 'pred_cm' 'pred_js'\n",
      " 'pred_cc' 'ref_id']\n",
      "Shape of the training file = (3779, 10)\n",
      "Columns of the predictions file = ['lat' 'lon' 'time' 'pred_av' 'pred_gs' 'pred_cm' 'pred_js' 'pred_cc'\n",
      " 'ref_id']\n",
      "Shape of the predictions file = (315210, 9)\n",
      "After sampling...\n",
      "Shape of the predictions file = (315210, 9)\n"
     ]
    }
   ],
   "source": [
    "#X_train, y_train, models_on_train, start_date = get_training_data_multi_time(trainDaily, 'obs_pm25', baseModelSet)\n",
    "#locations, predictions = get_prediction_data(predDaily, baseModelSet,  1, 0, start_date)\n",
    "# name the base models we will use and the dataset location\n",
    "baseModelSet_ann = ['av', 'gs','cm', 'js', 'cc']\n",
    "trainAnnualspt = 'BNE_inputs/training_datasets/' + \\\n",
    "'combined_annual/training_avgscmjscc_all.csv'\n",
    "predAnnualspt = 'BNE_inputs/prediction_datasets/' + \\\n",
    "'combined_annual/predictions_avgscmjscc_all.csv'\n",
    "\n",
    "X_train_ann, y_train_ann, models_on_train_ann, start_date_ann = get_training_data_multi_year(trainAnnualspt, 'obs_pm25', baseModelSet_ann)\n",
    "baseModelSet_ann\n",
    "locations_ann, predictions_ann = get_prediction_data_annual(predAnnualspt, baseModelSet_ann,  1, 0, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit BNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_map_ann, w0_map_ann, COV_Ann, rr_ann, LL_ann = fit_bne(X_train_ann, y_train_ann, models_on_train_ann, D=500, h=1.5, ht=0.5, sigmasq=-1.0, lmbda=1.0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BNE TFP Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
