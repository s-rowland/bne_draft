---
title: "Examining Time-Averaged Annual Uncertainty"
author: "Sebastian T. Rowland"
date: "8/23/2021"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# 0a Load package required for this script
if(!exists("Ran_a_00")){
  here::i_am("README.md")
  source(here::here('scripts', 'a_set_up', "a_00_set_up_env.R"))
}

if(!exists("Ran_a_00_ISEE")){
  here::i_am("README.md")
  source(here::here('uncertainty_factor_analysis_ISEE', 'scripts', 
                    "a_00_set_up_env_ISEE.R"))
}

# 0b Read CONUS shape file 
conus <-   st_read(here::here('data_ancillary', 'formatted', 'spatial_outlines', 
                              'conus.shp'))
```

# **Set Up**

## Problem Context 

Broadly, characterizing where we have greater uncertainty in our estimates of PM<sub>2.5</sub> concentration can help inform subsequent model development and monitor placement. Our model, the CONUS-BNE, combines several state-of-the-art PM<sub>2.5</sub> models; therefore its estimates should reflect the combined strengths of these models, and its uncertainty should reflect both uncertainty in how to combine the models, disagreement among the models, lack of data, and fundamental uncertainty (aleatoric uncertainty). It is important to note that a model's uncertainty always depends on the particular features of that model.  

There are two components of the uncertainty analysis: 1) qualitatively describing where uncertainty is the highest, including any clusters and 2) quantitatively assessing the relationship between uncertainty and _a priori_-selected potential explanatory factors.

For now we will examine results from the CONUS-BNE annual model because the daily model is not yet finalized. The model found very similar spatial patterns of uncertainty year to year; therefore we decided to average the years together into a single spatial dataset. 

## Set Up Data

We will first bring in the data I prepared in previous steps. This data includes PM<sub>2.5</sub> estimates and uncertainty from BNE, and the potential explanatory variables. The potential explanatory variables were conbined with the BNE estimates via nearest neighbors. We will then average the values across years. 
We will pull in the already-prepared data, for which we joined BNE estimates and potential explanatory variables via nearest-neighbors. We then average the values across years, since the spatial patterns of uncertainty and the potential explanatory variables are quite consistent across the years. 

```{r dataPrep, echo=FALSE}
# 1a Read Data
readCombinedData <- function(YYYY){
  read_fst(here::here('uncertainty_factor_analysis_ISEE', 'data', 'processed', 
                      paste0('AVGSCMJSCC_3.5_explanatory_', YYYY, '_025deg.fst'))) %>% 
    mutate(YYYY = YYYY) 
}
dta <- map(2010:2015, readCombinedData) %>% 
  bind_rows()
rm(readCombinedData)

# 1b Convert units of population density to thousand ppl per km 
dta <- dta %>% 
  mutate(pop_density_1kper1000km2 = pop_density*1000000 / 1000) %>% 
  mutate(lat = as.numeric(as.character(lat)), 
         lon = as.numeric(as.character(lon))) 

# 1d Average across years 
dta <- dta %>% 
  dplyr::select(-zcta, -state_num, -YYYY) %>%
  group_by(lat, lon, region, state) %>% 
  summarize_all(mean) %>% 
  ungroup() 
mp <- function(x){round(x, 2)}
# 1e Scale all of the variables... yes! 
# Let's not scale, it makes it a bit harder to interpret the plots
#dta <- dta %>% 
 #mutate_at(vars(-lat, -lon, -state, -region), scale) 

# remove rows with missing variables
dta <- dta %>% 
  filter(complete.cases(dta))

# Calculate distance to each near monitor
dta <- dta %>% 
  mutate(mon_dist1 = monDist1.mean, 
         mon_dist2 = 2*monDist2.mean-monDist1.mean, 
         mon_dist3 = 3*monDist3.mean-2*monDist2.mean, 
         mon_dist4 = 4*monDist4.mean-3*monDist3.mean, 
         mon_dist5 = 5*monDist5.mean-4*monDist4.mean)
```

# **Initial Data Exploration**

Let's then take a look at the distribution of predictive uncertainty. 

* We have a long right tail - there are locations with extremely high uncertainty, but not so much for low uncertainty. 

```{r unerDist, echo=FALSE}
print('Distribution of Predictive Uncertainty')
data.frame(mean = mean(dta$pred_sd), sd = sd(dta$pred_sd),
     min = min(dta$pred_sd), Q1 = quantile(dta$pred_sd, 0.25), 
     median = median(dta$pred_sd),Q3 = quantile(dta$pred_sd, 0.75), 
     max = max(dta$pred_sd)) %>% 
  mutate_all(mp)
```

## Crude Correlations

Let's look at the linear correlations among the explanatory variables, and with predictive uncertainty (pred_sd). 

* Predicted PM<sub>2.5</sub> concentration (pred_mean, the mean of the posterior predictive distribution of the CONUS-BNE model) was negatively correlated with predictive uncertainty 
* Predictive uncertainty was highly correlated with distance to the nearest monitor (mon_dist1), but not strongly correlated with any other potential explanatory variable. 
* However, these linear correlations may be masking non-linear relationships. 
* It is also interesting to note that predicted concentration is negatively correlated with distance to nearest monitor and elevation. 

```{r correlationExpVar}

dta.dist <- dta %>% 
  dplyr::select(pred_sd, pred_mean, mon_dist1,winter_temp, summer_temp, pop_density_1kper1000km2, cloud_cover, elev )
dta.dist.cor <- cor(dta.dist)

corrplot(dta.dist.cor, method = 'number', type = 'lower')
```

Next, let's consider alternative metrics of monitor proximity by plotting the correlation between distance to nearest monitor and predictive uncertainty. 

* The correlation between predictive uncertainty and distance to nearest monitor increases as we consider further away monitors (e.g., mon_dist2 is the distance to the second-nearest monitor). 
* This correlation plateaus after the fourth-nearest monitor. 
* One part of the explanation for this phenonenon is that the distance to the fourth-nearest monitor actually provides some information about the nearest monitor and second and third-nearest monitors. Specifically, the distance to the fourth-nearest monitor tells us the maximum possible distance of the 3 more-proximate monitors. For example, if the distance to the fourth-nearest monitor is 10 km, then the nearest monitor must also be within 10 km of the point.
* Additionally, low distance to fourth-nearest monitor indicates high monitor density. 
* These distances are highly correlated to each other (e.g., the correlation between distance to nearest monitor and second-nearest monitor is 0.89), so for now if we use distance to nearest monitor we will not lose much information. 

```{r correlationsmonDist}
dta.dist <- dta %>% 
  dplyr::select(pred_sd, mon_dist1, mon_dist2, mon_dist3, mon_dist4, mon_dist5)
dta.dist.cor <- cor(dta.dist)

corrplot(dta.dist.cor, method = 'number', type = 'lower')
```

# **Model Development**

## Check For Linearity 

Next let's fit a model with just one monitor-proximity metric, distance to nearest monitor, and then check for evidence of non-linear terms with penalized splines. 

* We see that population density has an estimated degree of freedom of 1, so we can model it as a linear term. 
* All other terms have edf >> 1, so we will model those as penalized splines

```{r selectTerms, cache =TRUE}
mod.allPsp <- gam(pred_sd ~ s(pred_mean) + s(mon_dist1)  +  s(winter_temp) + s(summer_temp) + s(pop_density_1kper1000km2) + s(cloud_cover) + s(elev) , data = dta)
summary(mod.allPsp)$s.table[,1]
```

## Fit Non-Spatial Model

We will first examine the relationship between the potential explanatory variables and uncertainty via simple GAMs that do not take into account the spatial structure of the data. We fit univariate models to examine the non-linear relationships between the potential explanatory factors and a multivariate GAM to examine the proportion of uncertainty explained by each variable. 

* We see that the distance to nearest monitor has positively monotonic relationship with predictive uncertainty, with a steeper slope above 300 km (where data is more sparse). 

```{r plotNonSpatial, fig.dim = c(4,4)}


plotUniVarUncertnonSp('mon_dist1')
plotUniVarUncertnonSp('pred_mean')
plotUniVarUncertnonSp('winter_temp')
plotUniVarUncertnonSp('summer_temp')
plotUniVarUncertnonSp('cloud_cover')
plotUniVarUncertnonSp('elev')

# also report the deviance explained.
mod.nonsp <- gam(pred_sd ~ s(pred_mean) + s(mon_dist1) +  s(winter_temp) + s(summer_temp)  + pop_density_1kper1000km2 + s(cloud_cover) + s(elev) , data = dta)


 mod.nonsp.noPredPM <- gam(pred_sd ~ s(mon_dist1) + s(winter_temp) + s(summer_temp) + pop_density_1kper1000km2 + s(cloud_cover) + s(elev), data = dta, 
                           sp = c(mod.nonsp$sp[2:6]))
mod.nonsp.noMonDist <- gam(pred_sd ~ s(pred_mean) + s(winter_temp) + s(summer_temp) + pop_density_1kper1000km2 + s(cloud_cover) + s(elev), data = dta, 
                           sp = c(mod.nonsp$sp[1], mod.nonsp$sp[3:6]))
mod.nonsp.noWinT <- gam(pred_sd ~ s(pred_mean) + s(mon_dist1) + s(summer_temp) + pop_density_1kper1000km2 + s(cloud_cover) + s(elev), data = dta, 
                           sp = c(mod.nonsp$sp[1:2], mod.nonsp$sp[4:6]))
mod.nonsp.noSumT <- gam(pred_sd ~ s(pred_mean) + s(mon_dist1) + s(winter_temp) + pop_density_1kper1000km2 + s(cloud_cover) + s(elev), data = dta, 
                           sp = c(mod.nonsp$sp[1:3], mod.nonsp$sp[5:6]))
mod.nonsp.noPopD <- gam(pred_sd ~ s(pred_mean) + s(mon_dist1) + s(winter_temp) + s(summer_temp) + s(cloud_cover) + s(elev), data = dta, 
                           sp = mod.nonsp$sp[1:6])
mod.nonsp.noCloud <- gam(pred_sd ~ s(pred_mean) + s(mon_dist1) + s(winter_temp) + s(summer_temp) + s(elev), data = dta, 
                           sp = c(mod.nonsp$sp[1:4], mod.nonsp$sp[6]))
mod.nonsp.noElev <- gam(pred_sd ~ s(pred_mean) + s(mon_dist1) + s(winter_temp) + s(summer_temp) + s(cloud_cover), data = dta, 
                           sp = mod.nonsp$sp[1:5])

propDev <- data.frame(
  var = c('pred_mean', 'mon_dist', 'winter_temp', 'summer_temp', 'pop_density', 
           'cloud', 'elev'), 
  prop_dev = c(
    calcPropDevExpl(mod.nonsp.noMonDist, mod.nonsp),
    calcPropDevExpl(mod.nonsp.noWinT, mod.nonsp), 
    calcPropDevExpl(mod.nonsp.noSumT, mod.nonsp), 
    calcPropDevExpl(mod.nonsp.noPredPM, mod.nonsp), 
    calcPropDevExpl(mod.nonsp.noCloud, mod.nonsp), 
    calcPropDevExpl(mod.nonsp.noElev, mod.nonsp),
    calcPropDevExpl(mod.nonsp.noPopD, mod.nonsp))) %>% 
  mutate(prop_dev = round(prop_dev, 3))

propDev

```


## Examine Residuals
So far, so good. However, our residuals may be correlated across space, violating the model's assumption that the residuals are independently and identically distributed (i.i.d). Let's check for spatial autocorrelation with Moran's i and a semi-variogram 

* There is statistically significant spatial correlation of the residuals, though the magnitude of the correlation is low.
* Our model's residuals have much lower spatial correlation than unadjusted uncertainty, 
indicating that our model captures many of the underlying spatial processes generating predictive uncertainty. 

```{r MoransI}
mI.nonSpatial <- calcMoransI(mod.nonsp$residuals, cbind(dta$lat, dta$lon))
print(paste0('Moran\'s I: ', round(mI.nonSpatial$observed, 5), '  SD: ', round(mI.nonSpatial$sd, 5), '  p value:', 
             round(mI.nonSpatial$p.value, 3)))
```

```{r Variogram,  warning = FALSE}
# convert to a Spatial points data frame 
# first convert to sf 
dta$resid <- mod.nonsp$residuals 
dta.sf <- dta %>% 
  st_as_sf( coords = c("lon", "lat"), 
            crs=st_crs(projString))
# next convert to SP 
dta.sp <- dta.sf %>% 
  as("Spatial")

prop_cor.default <- variogram(resid~1, data = dta.sp)  %>% 
  mutate(dist = dist / 1000) %>% 
  mutate(plot_name = 'default bins')
prop_cor.long <- variogram(resid~1, data = dta.sp, 
                            cutoff = 2704168, width = 121000)  %>% 
  mutate(dist = dist / 1000) 

plot(prop_cor.long, xlab = 'Distance (km)',
     main='Autocorrelation of Residuals')

prop_cor_fit.long <- fit.variogram(prop_cor.long, vgm(c("Sph","Exp","Mat", 'Ste')))
#plot(prop_cor.long, prop_cor_fit.long)
# note that results are pretty much the same when we use the default values of variogram()
```

```{r moransIPredSD,  warning = FALSE}
mI.predSD <- calcMoransI(dta$pred_sd, cbind(dta$lat, dta$lon))
print(paste0('Moran\'s I: ', round(mI.predSD$observed, 5), '  SD: ', round(mI.predSD$sd, 5), '  p value:', 
             round(mI.predSD$p.value, 3)))
```

## Initial Spatial Model: Geoadditive Model 

* A geoadditive model is a GAM with penalized spline terms for space. See () for more information, also mentioned briefly by Simon Wood in the documentation for mgcv, though he does not call it a geoadditive model. Specifically, we create a tensor product between latitude and longitude. Potentially we could extend this to a spatio-temporal model by adding a basis + interaction for time. 
* A major strength of the GAM is that it allows us to fit highly flexible curves to describe the relationships between the variables. 
* However, we will need to choose the number of knots for the spatial tensor product.     
    * Too few knots will fail to address the spatial autocorrelation, defeating the purpose of adding the tensor product
    * Too many knots could lead to a model where the spatial term explains variability that the explanatory factors could explain. This is due to the fact that the explanatory variables also vary smoothly in space, and in particular, the general pattern of uncertainty looks similar to the general pattern of one variable - distance to nearest monitor. 

* Model-fitting strategy: 
  1. Fit a model with the default number of knots. 
  2. Check that model for residual spatial autocorrelation 
  3. Increase the number of knots until the residual spatial autocorrelation is no longer significant. 
  4. Examine model 
  
And so what we find is that we need a lot of knots to get rid of the significant autocorrelation 

I do also wonder if this auto corr is so small that we can just like... ignore it? 

### Geoadditive Model with Default Knots 
 The default number of knots for a tensor product is 5 knots per basis, for a total of 5*5 = 25 maximum degrees of freedom. 
 
 * Adding the tensor product with 5 knots did not reduce the spatial correlation of the residuals

```{r geoadditive1,  warning = FALSE}
geoadd.defaultK <- gam(pred_sd ~ s(pred_mean) + s(monDist1.mean) + s(winter_temp) +
                     s(summer_temp) + pop_density_1kper1000km2 + 
                  s(cloud_cover) + s(elev) +
                  te(lat, lon),
                   data = dta)

mI.geoadd1 <- calcMoransI(geoadd.defaultK$residuals, cbind(dta$lat, dta$lon))
print(paste0('Moran\'s I: ', round(mI.geoadd1$observed, 5), '  SD: ', round(mI.geoadd1$sd, 5), '  p value:', 
             round(mI.geoadd1$p.value, 3)))
```

### Geoadditive Model with Minimal Sufficient Knots 
 After iterating over several possible number of knots, I found that 18 knots per basis was the sufficient number of knots to get rid of the spatial correlation of the residuals. 
 
 * This tensor product has 229 estimated degrees of freedom. 
 
```{r geoadditive2,  warning = FALSE}
geoadd.bigK <- gam(pred_sd ~ s(pred_mean) + s(monDist1.mean) + s(winter_temp) +
                     s(summer_temp) + pop_density_1kper1000km2 + 
                  s(cloud_cover) + s(elev) +
                  te(lat, lon, k = 18),
                   data = dta)

mI.geoadd2 <- calcMoransI(geoadd.bigK$residuals, cbind(dta$lat, dta$lon))
print(paste0('Moran\'s I: ', round(mI.geoadd2$observed, 5), '  SD: ', round(mI.geoadd2$sd, 5), '  p value:', 
             round(mI.geoadd2$p.value, 3)))
```

However, the relationship between distance to nearest monitor and predictive uncertainty has been obliterated. 

* This result indicates that a geoadditive model is not a good approach for this data. 

```{r plotgeoAdd}
geoadd.bigK.noMonDist <- gam(pred_sd ~ s(pred_mean) + s(winter_temp) +
                     s(summer_temp) + pop_density_1kper1000km2 + 
                  s(cloud_cover) + s(elev) +
                  te(lat, lon, k = 18), sp = c(mod.nonsp$sp[1], mod.nonsp$sp[3:6]),
                   data = dta)

pd <- calcPropDevExpl(modPartial = geoadd.bigK.noMonDist, 
                               modFull = geoadd.bigK)
print(paste0('Proportion Deviance explained by distance to nearest monitor: ', 
             round(pd, 3)))

```

## Final Spatial Model: Spatial, non-linear Model 


